{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4fb45e-9f32-4c4c-8a78-643dc905ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-04 00:02:52] ============================================================\n",
      "[2025-11-04 00:02:52] INICIANDO GENERACI√ìN DE MODELO ESTRELLA\n",
      "[2025-11-04 00:02:52] ============================================================\n",
      "[2025-11-04 00:02:52] \n",
      "1. Cargando CSV: D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\n",
      "[2025-11-04 00:02:54]    ‚úì 131,470 registros cargados\n",
      "[2025-11-04 00:02:54]    ‚úì 42 columnas\n",
      "[2025-11-04 00:02:54] \n",
      "2. Creando dimensiones...\n",
      "[2025-11-04 00:02:54] Creando DIM_TIEMPO...\n",
      "[2025-11-04 00:02:54]   ‚úì 120 registros creados\n",
      "[2025-11-04 00:02:54] Creando DIM_GEOGRAFIA...\n",
      "[2025-11-04 00:02:54]   ‚úì 8090 registros creados\n",
      "[2025-11-04 00:02:54] Creando DIM_HOGAR...\n",
      "[2025-11-04 00:02:54]   ‚úì 94921 registros creados\n",
      "[2025-11-04 00:02:54] Creando DIM_MADRE...\n",
      "[2025-11-04 00:02:54]   ‚úì 99928 registros creados\n",
      "[2025-11-04 00:02:54] Creando DIM_NINO...\n",
      "[2025-11-04 00:02:54]   ‚úì 131470 registros creados\n",
      "[2025-11-04 00:02:54] \n",
      "3. Creando tabla de hechos...\n",
      "[2025-11-04 00:02:54] Creando FACT_ANEMIA...\n",
      "[2025-11-04 00:02:55]   ‚úì 189846 registros creados\n",
      "[2025-11-04 00:02:55] \n",
      "4. Validando modelo...\n",
      "[2025-11-04 00:02:55] \n",
      "Validando integridad referencial...\n",
      "[2025-11-04 00:02:55]   ‚úì id_tiempo: OK\n",
      "[2025-11-04 00:02:55]   ‚úì id_geografia: OK\n",
      "[2025-11-04 00:02:55]   ‚úì id_hogar: OK\n",
      "[2025-11-04 00:02:55]   ‚úì id_madre: OK\n",
      "[2025-11-04 00:02:55]   ‚úì id_nino: OK\n",
      "[2025-11-04 00:02:55]   ‚úÖ Integridad referencial OK\n",
      "[2025-11-04 00:02:55] \n",
      "Validando calidad de datos...\n",
      "[2025-11-04 00:02:55]   Total registros FACT: 189,846\n",
      "[2025-11-04 00:02:55]   ‚ö†Ô∏è  Duplicados encontrados: 54501\n",
      "[2025-11-04 00:02:55] \n",
      "Dimensiones:\n",
      "[2025-11-04 00:02:55]   DIM_TIEMPO:       120 registros\n",
      "[2025-11-04 00:02:55]   DIM_GEOGRAFIA:  8,090 registros\n",
      "[2025-11-04 00:02:55]   DIM_HOGAR:     94,921 registros\n",
      "[2025-11-04 00:02:55]   DIM_MADRE:     99,928 registros\n",
      "[2025-11-04 00:02:55]   DIM_NINO:      131,470 registros\n",
      "[2025-11-04 00:02:55] \n",
      "  ‚úÖ Validaci√≥n de calidad completada\n",
      "[2025-11-04 00:02:55] \n",
      "5. Cargando a Data Warehouse...\n",
      "[2025-11-04 00:02:55] \n",
      "Guardando en SQLite: D:\\Data_Warehouse\\anemia_dwh.db\n",
      "[2025-11-04 00:02:55] \n",
      "Creando esquema SQLite...\n",
      "[2025-11-04 00:02:55]   ‚úì Tablas anteriores eliminadas (si exist√≠an)\n",
      "[2025-11-04 00:02:55]   Guardando dimensiones...\n",
      "[2025-11-04 00:02:58]   Guardando tabla de hechos...\n",
      "[2025-11-04 00:03:01]   Creando √≠ndices...\n",
      "[2025-11-04 00:03:04]   ‚úÖ Guardado en SQLite completado\n",
      "[2025-11-04 00:03:04] \n",
      "Creando backup Parquet en D:\\Data_Warehouse\\backup...\n",
      "[2025-11-04 00:03:06]   ‚úÖ Backup Parquet completado\n",
      "[2025-11-04 00:03:06] \n",
      "Generando metadata...\n",
      "[2025-11-04 00:03:06]   ‚úì Metadata guardada\n",
      "[2025-11-04 00:03:06] \n",
      "============================================================\n",
      "[2025-11-04 00:03:06] ‚úÖ MODELO ESTRELLA GENERADO EXITOSAMENTE\n",
      "[2025-11-04 00:03:06] ============================================================\n",
      "[2025-11-04 00:03:06] \n",
      "üìÅ Archivos generados:\n",
      "[2025-11-04 00:03:06]    ‚Ä¢ SQLite:  D:\\Data_Warehouse\\anemia_dwh.db\n",
      "[2025-11-04 00:03:06]    ‚Ä¢ Metadata: D:\\Data_Warehouse\\metadata\n",
      "[2025-11-04 00:03:06]    ‚Ä¢ Backup:   D:\\Data_Warehouse\\backup\n",
      "[2025-11-04 00:03:06] \n",
      "üìä Estad√≠sticas:\n",
      "[2025-11-04 00:03:06]    ‚Ä¢ Tama√±o DB: 44.27 MB\n",
      "[2025-11-04 00:03:06]    ‚Ä¢ Total registros FACT: 189,846\n",
      "[2025-11-04 00:03:06]    ‚Ä¢ Prevalencia anemia: 45.52%\n",
      "[2025-11-04 00:03:06] \n",
      "============================================================\n",
      "[2025-11-04 00:03:06] Proceso completado en 14.10 segundos\n",
      "[2025-11-04 00:03:06] ============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GENERADOR DE MODELO ESTRELLA - ANEMIA INFANTIL ENDES\n",
    "Convierte CSV consolidado ‚Üí SQLite con esquema estrella\n",
    "Autor: [Tu nombre]\n",
    "Fecha: 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "class Config:\n",
    "    # Rutas\n",
    "    CSV_INPUT = r\"D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\"\n",
    "    DB_OUTPUT = r\"D:\\Data_Warehouse\\anemia_dwh.db\"\n",
    "    METADATA_DIR = Path(r\"D:\\Data_Warehouse\\metadata\")\n",
    "    BACKUP_DIR = Path(r\"D:\\Data_Warehouse\\backup\")\n",
    "    \n",
    "    # Crear directorios si no existen\n",
    "    METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Configuraci√≥n de procesamiento\n",
    "    CHUNK_SIZE = 5000  # Para inserci√≥n en SQLite\n",
    "    BACKUP_PARQUET = True  # Crear backup en parquet\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================\n",
    "# UTILIDADES\n",
    "# ============================================================\n",
    "class Logger:\n",
    "    \"\"\"Logger simple para ETL\"\"\"\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "    def log(self, mensaje):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        msg = f\"[{timestamp}] {mensaje}\"\n",
    "        print(msg)\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(msg + '\\n')\n",
    "    \n",
    "    def finalizar(self):\n",
    "        duracion = (datetime.now() - self.start_time).total_seconds()\n",
    "        self.log(f\"\\n{'='*60}\")\n",
    "        self.log(f\"Proceso completado en {duracion:.2f} segundos\")\n",
    "        self.log(f\"{'='*60}\")\n",
    "\n",
    "# Inicializar logger\n",
    "log_file = config.METADATA_DIR / f\"log_etl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logger = Logger(log_file)\n",
    "\n",
    "# ============================================================\n",
    "# FUNCIONES DE DIMENSIONES\n",
    "# ============================================================\n",
    "\n",
    "def crear_dim_tiempo(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n TIEMPO\n",
    "    Granularidad: A√±o (simplificado para ENDES)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_TIEMPO...\")\n",
    "    \n",
    "    anios = sorted(df['ANIO'].unique())\n",
    "    \n",
    "    dim_tiempo = []\n",
    "    for anio in anios:\n",
    "        for mes in range(1, 13):\n",
    "            dim_tiempo.append({\n",
    "                'id_tiempo': int(f\"{anio}{mes:02d}\"),  # 202401\n",
    "                'anio': int(anio),\n",
    "                'mes': mes,\n",
    "                'trimestre': (mes - 1) // 3 + 1,\n",
    "                'semestre': 1 if mes <= 6 else 2,\n",
    "                'nombre_mes': ['Enero','Febrero','Marzo','Abril','Mayo','Junio',\n",
    "                              'Julio','Agosto','Septiembre','Octubre','Noviembre','Diciembre'][mes-1],\n",
    "                'quinquenio': f\"{(anio//5)*5}-{(anio//5)*5+4}\",\n",
    "                'periodo_covid': int(anio in [2020, 2021])\n",
    "            })\n",
    "    \n",
    "    df_tiempo = pd.DataFrame(dim_tiempo)\n",
    "    logger.log(f\"  ‚úì {len(df_tiempo)} registros creados\")\n",
    "    return df_tiempo\n",
    "\n",
    "\n",
    "def crear_dim_geografia(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n GEOGRAF√çA\n",
    "    Combina: departamento + √°rea + altitud\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_GEOGRAFIA...\")\n",
    "    \n",
    "    # Seleccionar columnas geogr√°ficas\n",
    "    geo_cols = ['HV024', 'HV025', 'HV040']\n",
    "    \n",
    "    # Combinaciones √∫nicas\n",
    "    df_geo = df[geo_cols].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_geo['id_geografia'] = range(1, len(df_geo) + 1)\n",
    "    \n",
    "    # Mapeo regi√≥n natural (simplificado - ajustar seg√∫n necesidad)\n",
    "    mapa_region = {\n",
    "        'tumbes': 'Costa', 'piura': 'Costa', 'lambayeque': 'Costa', \n",
    "        'la libertad': 'Costa', 'ancash': 'Costa', 'lima': 'Costa',\n",
    "        'ica': 'Costa', 'arequipa': 'Costa', 'moquegua': 'Costa', 'tacna': 'Costa',\n",
    "        'cajamarca': 'Sierra', 'huanuco': 'Sierra', 'pasco': 'Sierra',\n",
    "        'junin': 'Sierra', 'huancavelica': 'Sierra', 'ayacucho': 'Sierra',\n",
    "        'apurimac': 'Sierra', 'cusco': 'Sierra', 'puno': 'Sierra',\n",
    "        'loreto': 'Selva', 'amazonas': 'Selva', 'san martin': 'Selva',\n",
    "        'ucayali': 'Selva', 'madre de dios': 'Selva'\n",
    "    }\n",
    "    \n",
    "    df_geo['region_natural'] = df_geo['HV024'].str.lower().map(mapa_region)\n",
    "    df_geo['region_natural'] = df_geo['region_natural'].fillna('Otro')\n",
    "    \n",
    "    # Categorizar altitud\n",
    "    df_geo['rango_altitud'] = pd.cut(\n",
    "        df_geo['HV040'],\n",
    "        bins=[-1, 500, 1500, 2500, 5000],\n",
    "        labels=['<500m', '500-1500m', '1500-2500m', '>2500m']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    df_geo.rename(columns={\n",
    "        'HV024': 'departamento',\n",
    "        'HV025': 'area_residencia',\n",
    "        'HV040': 'altitud_msnm'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Ordenar columnas\n",
    "    df_geo = df_geo[['id_geografia', 'departamento', 'region_natural', \n",
    "                     'area_residencia', 'altitud_msnm', 'rango_altitud']]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_geo)} registros creados\")\n",
    "    return df_geo\n",
    "\n",
    "\n",
    "def crear_dim_hogar(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n HOGAR\n",
    "    Granularidad: HHID √∫nico\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_HOGAR...\")\n",
    "    \n",
    "    # Columnas de hogar\n",
    "    hogar_cols = ['HHID', 'HV009', 'HV271', 'V190', \n",
    "                  'HV206', 'HV201', 'HV205', 'HV237']\n",
    "    \n",
    "    # Verificar columnas disponibles\n",
    "    hogar_cols_disponibles = [col for col in hogar_cols if col in df.columns]\n",
    "    \n",
    "    # Extraer hogares √∫nicos\n",
    "    df_hogar = df[hogar_cols_disponibles].drop_duplicates(subset='HHID').reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_hogar['id_hogar'] = range(1, len(df_hogar) + 1)\n",
    "    \n",
    "    # Crear categor√≠as si las columnas existen\n",
    "    if 'HV009' in df_hogar.columns:\n",
    "        df_hogar['categoria_tamano'] = pd.cut(\n",
    "            df_hogar['HV009'],\n",
    "            bins=[0, 3, 5, 100],\n",
    "            labels=['Peque√±o', 'Mediano', 'Grande']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HV009': 'num_miembros',\n",
    "        'HV271': 'indice_riqueza_num',\n",
    "        'V190': 'quintil_riqueza',\n",
    "        'HV206': 'tiene_electricidad',\n",
    "        'HV201': 'fuente_agua',\n",
    "        'HV205': 'tipo_saneamiento',\n",
    "        'HV237': 'trata_agua'\n",
    "    }\n",
    "    \n",
    "    df_hogar.rename(columns={k: v for k, v in rename_map.items() if k in df_hogar.columns}, \n",
    "                    inplace=True)\n",
    "    \n",
    "    # Mover id_hogar y HHID al inicio\n",
    "    cols = ['id_hogar', 'HHID'] + [col for col in df_hogar.columns if col not in ['id_hogar', 'HHID']]\n",
    "    df_hogar = df_hogar[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_hogar)} registros creados\")\n",
    "    return df_hogar\n",
    "\n",
    "\n",
    "def crear_dim_madre(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n MADRE\n",
    "    Granularidad: CASEID √∫nico\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_MADRE...\")\n",
    "    \n",
    "    # Columnas de madre\n",
    "    madre_cols = ['CASEID', 'V012', 'V106', 'V133', 'V025']\n",
    "    madre_cols_disponibles = [col for col in madre_cols if col in df.columns]\n",
    "    \n",
    "    # Extraer madres √∫nicas\n",
    "    df_madre = df[madre_cols_disponibles].drop_duplicates(subset='CASEID').reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_madre['id_madre'] = range(1, len(df_madre) + 1)\n",
    "    \n",
    "    # Crear categor√≠as si existen las columnas\n",
    "    if 'V012' in df_madre.columns:\n",
    "        df_madre['rango_edad'] = pd.cut(\n",
    "            df_madre['V012'],\n",
    "            bins=[0, 20, 35, 100],\n",
    "            labels=['<20', '20-34', '35+']\n",
    "        ).astype(str)\n",
    "        \n",
    "        df_madre['es_madre_adolescente'] = (df_madre['V012'] < 20).astype(int)\n",
    "    \n",
    "    if 'V133' in df_madre.columns:\n",
    "        df_madre['categoria_educacion'] = pd.cut(\n",
    "            df_madre['V133'],\n",
    "            bins=[-1, 6, 12, 100],\n",
    "            labels=['Baja', 'Media', 'Alta']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar\n",
    "    rename_map = {\n",
    "        'V012': 'edad_actual',\n",
    "        'V106': 'nivel_educativo',\n",
    "        'V133': 'anios_educacion',\n",
    "        'V025': 'area_residencia_madre'\n",
    "    }\n",
    "    \n",
    "    df_madre.rename(columns={k: v for k, v in rename_map.items() if k in df_madre.columns}, \n",
    "                    inplace=True)\n",
    "    \n",
    "    # Reordenar\n",
    "    cols = ['id_madre', 'CASEID'] + [col for col in df_madre.columns if col not in ['id_madre', 'CASEID']]\n",
    "    df_madre = df_madre[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_madre)} registros creados\")\n",
    "    return df_madre\n",
    "\n",
    "\n",
    "def crear_dim_nino(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n NI√ëO\n",
    "    Granularidad: HHID + HC0 (cada ni√±o)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_NINO...\")\n",
    "    \n",
    "    # Cada registro es un ni√±o √∫nico\n",
    "    nino_cols = ['HHID', 'HC0', 'HC1', 'HC27', 'BORD', 'HC70', 'HW71']\n",
    "    nino_cols_disponibles = [col for col in nino_cols if col in df.columns]\n",
    "    \n",
    "    df_nino = df[nino_cols_disponibles].copy()\n",
    "    \n",
    "    # Crear ID √∫nico combinando HHID + HC0\n",
    "    df_nino['id_nino'] = range(1, len(df_nino) + 1)\n",
    "    \n",
    "    # Crear categor√≠as\n",
    "    if 'HC1' in df_nino.columns:\n",
    "        df_nino['rango_edad'] = pd.cut(\n",
    "            df_nino['HC1'],\n",
    "            bins=[5, 12, 18, 24, 36],\n",
    "            labels=['6-11m', '12-17m', '18-23m', '24-35m'],\n",
    "            right=False\n",
    "        ).astype(str)\n",
    "    \n",
    "    if 'HC70' in df_nino.columns:\n",
    "        df_nino['tiene_desnutricion_cronica'] = (df_nino['HC70'] < -2).astype(int)\n",
    "    \n",
    "    if 'HW71' in df_nino.columns:\n",
    "        df_nino['tiene_bajo_peso'] = (df_nino['HW71'] < -2).astype(int)\n",
    "    \n",
    "    if 'BORD' in df_nino.columns:\n",
    "        df_nino['categoria_orden'] = pd.cut(\n",
    "            df_nino['BORD'],\n",
    "            bins=[0, 1, 3, 100],\n",
    "            labels=['Primog√©nito', '2-3', '4+']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar\n",
    "    rename_map = {\n",
    "        'HC1': 'edad_meses',\n",
    "        'HC27': 'sexo',\n",
    "        'BORD': 'orden_nacimiento'\n",
    "    }\n",
    "    \n",
    "    df_nino.rename(columns={k: v for k, v in rename_map.items() if k in df_nino.columns}, \n",
    "                   inplace=True)\n",
    "    \n",
    "    # Reordenar\n",
    "    cols = ['id_nino', 'HHID', 'HC0'] + [col for col in df_nino.columns \n",
    "                                          if col not in ['id_nino', 'HHID', 'HC0']]\n",
    "    df_nino = df_nino[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_nino)} registros creados\")\n",
    "    return df_nino\n",
    "\n",
    "\n",
    "def crear_fact_anemia(df, dim_tiempo, dim_geo, dim_hogar, dim_madre, dim_nino):\n",
    "    \"\"\"\n",
    "    Tabla de HECHOS - FACT_ANEMIA\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando FACT_ANEMIA...\")\n",
    "    \n",
    "    fact = df.copy()\n",
    "    \n",
    "    # 1. Mapear FK - TIEMPO (asignar a junio de cada a√±o)\n",
    "    fact['id_tiempo'] = fact['ANIO'].astype(int) * 100 + 6\n",
    "    \n",
    "    # 2. Mapear FK - GEOGRAFIA\n",
    "    fact = fact.merge(\n",
    "        dim_geo[['id_geografia', 'departamento', 'area_residencia', 'altitud_msnm']],\n",
    "        left_on=['HV024', 'HV025', 'HV040'],\n",
    "        right_on=['departamento', 'area_residencia', 'altitud_msnm'],\n",
    "        how='left'\n",
    "    ).drop(columns=['departamento', 'area_residencia', 'altitud_msnm'])\n",
    "    \n",
    "    # 3. Mapear FK - HOGAR\n",
    "    fact = fact.merge(\n",
    "        dim_hogar[['id_hogar', 'HHID']],\n",
    "        on='HHID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 4. Mapear FK - MADRE\n",
    "    fact = fact.merge(\n",
    "        dim_madre[['id_madre', 'CASEID']],\n",
    "        on='CASEID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 5. Mapear FK - NINO\n",
    "    fact = fact.merge(\n",
    "        dim_nino[['id_nino', 'HHID', 'HC0']],\n",
    "        on=['HHID', 'HC0'],\n",
    "        how='left',\n",
    "        suffixes=('', '_nino')\n",
    "    )\n",
    "    \n",
    "    # 6. Seleccionar columnas de FACT\n",
    "    fact_cols = [\n",
    "        # FK\n",
    "        'id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre',\n",
    "        \n",
    "        # IDs originales (trazabilidad)\n",
    "        'HHID', 'CASEID', 'HC0',\n",
    "        \n",
    "        # M√©tricas antropom√©tricas\n",
    "        'HW2', 'HW3', 'HC70', 'HW70', 'HW71', 'HW72', 'HW73',\n",
    "        \n",
    "        # Target\n",
    "        'ANEMIA', 'HC57',\n",
    "        \n",
    "        # Pesos\n",
    "        'PESO',\n",
    "        \n",
    "        # Flags de calidad\n",
    "        'HC55', 'HV015', 'HV103'\n",
    "    ]\n",
    "    \n",
    "    # Filtrar solo columnas existentes\n",
    "    fact_cols_disponibles = [col for col in fact_cols if col in fact.columns]\n",
    "    fact = fact[fact_cols_disponibles]\n",
    "    \n",
    "    # 7. Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HW2': 'peso_kg',\n",
    "        'HW3': 'talla_cm',\n",
    "        'HC70': 'z_talla_edad',\n",
    "        'HW70': 'z_talla_edad_alt',\n",
    "        'HW71': 'z_peso_edad',\n",
    "        'HW72': 'z_peso_talla',\n",
    "        'HW73': 'z_imc',\n",
    "        'ANEMIA': 'tiene_anemia',\n",
    "        'HC57': 'nivel_anemia',\n",
    "        'PESO': 'peso_muestral',\n",
    "        'HC55': 'medicion_valida',\n",
    "        'HV015': 'cuestionario_ok',\n",
    "        'HV103': 'durmio_anoche'\n",
    "    }\n",
    "    \n",
    "    fact.rename(columns={k: v for k, v in rename_map.items() if k in fact.columns}, \n",
    "                inplace=True)\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(fact)} registros creados\")\n",
    "    \n",
    "    return fact\n",
    "\n",
    "# ============================================================\n",
    "# VALIDACIONES\n",
    "# ============================================================\n",
    "\n",
    "def validar_integridad_referencial(fact, dims):\n",
    "    \"\"\"Validar FK en tabla de hechos\"\"\"\n",
    "    logger.log(\"\\nValidando integridad referencial...\")\n",
    "    \n",
    "    errores = []\n",
    "    \n",
    "    # Verificar cada FK\n",
    "    checks = [\n",
    "        ('id_tiempo', dims['dim_tiempo'], 'id_tiempo'),\n",
    "        ('id_geografia', dims['dim_geografia'], 'id_geografia'),\n",
    "        ('id_hogar', dims['dim_hogar'], 'id_hogar'),\n",
    "        ('id_madre', dims['dim_madre'], 'id_madre'),\n",
    "        ('id_nino', dims['dim_nino'], 'id_nino')\n",
    "    ]\n",
    "    \n",
    "    for fk_col, dim_df, dim_pk in checks:\n",
    "        if fk_col in fact.columns:\n",
    "            invalidos = ~fact[fk_col].isin(dim_df[dim_pk])\n",
    "            n_invalidos = invalidos.sum()\n",
    "            \n",
    "            if n_invalidos > 0:\n",
    "                errores.append(f\"  ‚ùå {fk_col}: {n_invalidos} FK inv√°lidos\")\n",
    "            else:\n",
    "                logger.log(f\"  ‚úì {fk_col}: OK\")\n",
    "    \n",
    "    if errores:\n",
    "        logger.log(\"\\n‚ö†Ô∏è  ERRORES DE INTEGRIDAD ENCONTRADOS:\")\n",
    "        for error in errores:\n",
    "            logger.log(error)\n",
    "        raise ValueError(\"Integridad referencial violada\")\n",
    "    else:\n",
    "        logger.log(\"  ‚úÖ Integridad referencial OK\")\n",
    "\n",
    "\n",
    "def validar_calidad_datos(fact, dims):\n",
    "    \"\"\"Validar calidad de datos\"\"\"\n",
    "    logger.log(\"\\nValidando calidad de datos...\")\n",
    "    \n",
    "    # 1. Registros en fact\n",
    "    logger.log(f\"  Total registros FACT: {len(fact):,}\")\n",
    "    \n",
    "    # 2. Duplicados\n",
    "    duplicados = fact.duplicated(subset=['id_nino', 'id_tiempo']).sum()\n",
    "    if duplicados > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Duplicados encontrados: {duplicados}\")\n",
    "    else:\n",
    "        logger.log(f\"  ‚úì Sin duplicados\")\n",
    "    \n",
    "    # 3. Missings en FK\n",
    "    for col in ['id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre']:\n",
    "        if col in fact.columns:\n",
    "            missing = fact[col].isna().sum()\n",
    "            if missing > 0:\n",
    "                logger.log(f\"  ‚ö†Ô∏è  {col}: {missing} missings ({missing/len(fact)*100:.1f}%)\")\n",
    "    \n",
    "    # 4. Estad√≠sticas de dimensiones\n",
    "    logger.log(f\"\\nDimensiones:\")\n",
    "    logger.log(f\"  DIM_TIEMPO:    {len(dims['dim_tiempo']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_GEOGRAFIA: {len(dims['dim_geografia']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_HOGAR:     {len(dims['dim_hogar']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_MADRE:     {len(dims['dim_madre']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_NINO:      {len(dims['dim_nino']):>6,} registros\")\n",
    "    \n",
    "    logger.log(\"\\n  ‚úÖ Validaci√≥n de calidad completada\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR EN SQLITE\n",
    "# ============================================================\n",
    "\n",
    "def crear_esquema_sqlite(conn):\n",
    "    \"\"\"Crear estructura de tablas con DDL\"\"\"\n",
    "    logger.log(\"\\nCreando esquema SQLite...\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # DDL para cada tabla (simplificado - SQLite infiere tipos)\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS fact_anemia\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_tiempo\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_geografia\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_hogar\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_madre\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_nino\")\n",
    "    \n",
    "    conn.commit()\n",
    "    logger.log(\"  ‚úì Tablas anteriores eliminadas (si exist√≠an)\")\n",
    "\n",
    "\n",
    "def guardar_en_sqlite(fact, dims, db_path):\n",
    "    \"\"\"Guardar modelo estrella en SQLite\"\"\"\n",
    "    logger.log(f\"\\nGuardando en SQLite: {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Crear esquema\n",
    "    crear_esquema_sqlite(conn)\n",
    "    \n",
    "    # Guardar dimensiones\n",
    "    logger.log(\"  Guardando dimensiones...\")\n",
    "    dims['dim_tiempo'].to_sql('dim_tiempo', conn, if_exists='replace', index=False)\n",
    "    dims['dim_geografia'].to_sql('dim_geografia', conn, if_exists='replace', index=False)\n",
    "    dims['dim_hogar'].to_sql('dim_hogar', conn, if_exists='replace', index=False)\n",
    "    dims['dim_madre'].to_sql('dim_madre', conn, if_exists='replace', index=False)\n",
    "    dims['dim_nino'].to_sql('dim_nino', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Guardar hechos (en chunks)\n",
    "    logger.log(\"  Guardando tabla de hechos...\")\n",
    "    fact.to_sql('fact_anemia', conn, if_exists='replace', \n",
    "                index=False, chunksize=config.CHUNK_SIZE)\n",
    "    \n",
    "    # Crear √≠ndices\n",
    "    logger.log(\"  Creando √≠ndices...\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    indices = [\n",
    "        \"CREATE INDEX idx_fact_tiempo ON fact_anemia(id_tiempo)\",\n",
    "        \"CREATE INDEX idx_fact_geografia ON fact_anemia(id_geografia)\",\n",
    "        \"CREATE INDEX idx_fact_hogar ON fact_anemia(id_hogar)\",\n",
    "        \"CREATE INDEX idx_fact_madre ON fact_anemia(id_madre)\",\n",
    "        \"CREATE INDEX idx_fact_nino ON fact_anemia(id_nino)\",\n",
    "        \"CREATE INDEX idx_fact_anemia ON fact_anemia(tiene_anemia)\",\n",
    "        \"CREATE INDEX idx_dim_geo_dept ON dim_geografia(departamento)\",\n",
    "        \"CREATE INDEX idx_dim_tiempo_anio ON dim_tiempo(anio)\"\n",
    "    ]\n",
    "    \n",
    "    for idx_sql in indices:\n",
    "        try:\n",
    "            cursor.execute(idx_sql)\n",
    "        except Exception as e:\n",
    "            logger.log(f\"    ‚ö†Ô∏è  Error creando √≠ndice: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logger.log(\"  ‚úÖ Guardado en SQLite completado\")\n",
    "\n",
    "\n",
    "def guardar_backup_parquet(fact, dims):\n",
    "    \"\"\"Backup en Parquet\"\"\"\n",
    "    if not config.BACKUP_PARQUET:\n",
    "        return\n",
    "    \n",
    "    logger.log(f\"\\nCreando backup Parquet en {config.BACKUP_DIR}...\")\n",
    "    \n",
    "    fact.to_parquet(config.BACKUP_DIR / \"fact_anemia.parquet\", index=False)\n",
    "    dims['dim_tiempo'].to_parquet(config.BACKUP_DIR / \"dim_tiempo.parquet\", index=False)\n",
    "    dims['dim_geografia'].to_parquet(config.BACKUP_DIR / \"dim_geografia.parquet\", index=False)\n",
    "    dims['dim_hogar'].to_parquet(config.BACKUP_DIR / \"dim_hogar.parquet\", index=False)\n",
    "    dims['dim_madre'].to_parquet(config.BACKUP_DIR / \"dim_madre.parquet\", index=False)\n",
    "    dims['dim_nino'].to_parquet(config.BACKUP_DIR / \"dim_nino.parquet\", index=False)\n",
    "    \n",
    "    logger.log(\"  ‚úÖ Backup Parquet completado\")\n",
    "\n",
    "\n",
    "def generar_metadata(fact, dims):\n",
    "    \"\"\"Generar metadata del warehouse\"\"\"\n",
    "    logger.log(\"\\nGenerando metadata...\")\n",
    "    \n",
    "    metadata = {\n",
    "        'fecha_creacion': datetime.now().isoformat(),\n",
    "        'registros': {\n",
    "            'fact_anemia': len(fact),\n",
    "            'dim_tiempo': len(dims['dim_tiempo']),\n",
    "            'dim_geografia': len(dims['dim_geografia']),\n",
    "            'dim_hogar': len(dims['dim_hogar']),\n",
    "            'dim_madre': len(dims['dim_madre']),\n",
    "            'dim_nino': len(dims['dim_nino'])\n",
    "        },\n",
    "        'columnas': {\n",
    "            'fact_anemia': list(fact.columns),\n",
    "            'dim_tiempo': list(dims['dim_tiempo'].columns),\n",
    "            'dim_geografia': list(dims['dim_geografia'].columns),\n",
    "            'dim_hogar': list(dims['dim_hogar'].columns),\n",
    "            'dim_madre': list(dims['dim_madre'].columns),\n",
    "            'dim_nino': list(dims['dim_nino'].columns)\n",
    "        },\n",
    "        'prevalencia_anemia': float(fact['tiene_anemia'].mean() * 100) if 'tiene_anemia' in fact.columns else None\n",
    "    }\n",
    "    \n",
    "    # Guardar como JSON\n",
    "    with open(config.METADATA_DIR / 'estadisticas.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logger.log(\"  ‚úì Metadata guardada\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline completo ETL\"\"\"\n",
    "    \n",
    "    logger.log(\"=\"*60)\n",
    "    logger.log(\"INICIANDO GENERACI√ìN DE MODELO ESTRELLA\")\n",
    "    logger.log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. EXTRACT\n",
    "        logger.log(f\"\\n1. Cargando CSV: {config.CSV_INPUT}\")\n",
    "        df = pd.read_csv(config.CSV_INPUT, encoding='utf-8-sig')\n",
    "        logger.log(f\"   ‚úì {len(df):,} registros cargados\")\n",
    "        logger.log(f\"   ‚úì {df.shape[1]} columnas\")\n",
    "        \n",
    "        # 2. TRANSFORM - Crear dimensiones\n",
    "        logger.log(\"\\n2. Creando dimensiones...\")\n",
    "        dim_tiempo = crear_dim_tiempo(df)\n",
    "        dim_geografia = crear_dim_geografia(df)\n",
    "        dim_hogar = crear_dim_hogar(df)\n",
    "        dim_madre = crear_dim_madre(df)\n",
    "        dim_nino = crear_dim_nino(df)\n",
    "        \n",
    "        dims = {\n",
    "            'dim_tiempo': dim_tiempo,\n",
    "            'dim_geografia': dim_geografia,\n",
    "            'dim_hogar': dim_hogar,\n",
    "            'dim_madre': dim_madre,\n",
    "            'dim_nino': dim_nino\n",
    "        }\n",
    "        \n",
    "        # 3. TRANSFORM - Crear hechos\n",
    "        logger.log(\"\\n3. Creando tabla de hechos...\")\n",
    "        fact = crear_fact_anemia(df, dim_tiempo, dim_geografia, \n",
    "                                dim_hogar, dim_madre, dim_nino)\n",
    "        \n",
    "        # 4. VALIDACIONES\n",
    "        logger.log(\"\\n4. Validando modelo...\")\n",
    "        validar_integridad_referencial(fact, dims)\n",
    "        validar_calidad_datos(fact, dims)\n",
    "        \n",
    "        # 5. LOAD\n",
    "        logger.log(\"\\n5. Cargando a Data Warehouse...\")\n",
    "        guardar_en_sqlite(fact, dims, config.DB_OUTPUT)\n",
    "        guardar_backup_parquet(fact, dims)\n",
    "        generar_metadata(fact, dims)\n",
    "        \n",
    "        # FIN\n",
    "        logger.log(\"\\n\" + \"=\"*60)\n",
    "        logger.log(\"‚úÖ MODELO ESTRELLA GENERADO EXITOSAMENTE\")\n",
    "        logger.log(\"=\"*60)\n",
    "        logger.log(f\"\\nüìÅ Archivos generados:\")\n",
    "        logger.log(f\"   ‚Ä¢ SQLite:  {config.DB_OUTPUT}\")\n",
    "        logger.log(f\"   ‚Ä¢ Metadata: {config.METADATA_DIR}\")\n",
    "        logger.log(f\"   ‚Ä¢ Backup:   {config.BACKUP_DIR}\")\n",
    "        \n",
    "        # Estad√≠sticas finales\n",
    "        tamanio_db = Path(config.DB_OUTPUT).stat().st_size / (1024*1024)\n",
    "        logger.log(f\"\\nüìä Estad√≠sticas:\")\n",
    "        logger.log(f\"   ‚Ä¢ Tama√±o DB: {tamanio_db:.2f} MB\")\n",
    "        logger.log(f\"   ‚Ä¢ Total registros FACT: {len(fact):,}\")\n",
    "        logger.log(f\"   ‚Ä¢ Prevalencia anemia: {fact['tiene_anemia'].mean()*100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.log(traceback.format_exc())\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        logger.finalizar()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fa345-e1dc-4a4f-8ce7-a714e46d9a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b294023-8bea-4103-9615-446a2ef61b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33689cdc-7731-47c7-9289-9937d7802eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf0053-8f77-4077-bc54-6bc6a1f93755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8e0fc-870f-41c6-8696-500ba354f566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad01cfd-458b-44ed-bc84-a5f028224452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ae8341b-3224-42b2-b963-f2074ec43e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-04 01:32:11] ============================================================\n",
      "[2025-11-04 01:32:11] INICIANDO GENERACI√ìN DE MODELO ESTRELLA (VERSI√ìN CORREGIDA)\n",
      "[2025-11-04 01:32:11] ============================================================\n",
      "[2025-11-04 01:32:11] \n",
      "1. Cargando CSV: D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\n",
      "[2025-11-04 01:32:11]    ‚úì 131,470 registros cargados\n",
      "[2025-11-04 01:32:11]    ‚úì 42 columnas\n",
      "[2025-11-04 01:32:11] \n",
      "2. Creando dimensiones...\n",
      "[2025-11-04 01:32:11] Creando DIM_TIEMPO...\n",
      "[2025-11-04 01:32:11]   ‚úì 10 registros creados (1 por a√±o)\n",
      "[2025-11-04 01:32:11] Creando DIM_GEOGRAFIA...\n",
      "[2025-11-04 01:32:12]   ‚úì 8090 registros creados\n",
      "[2025-11-04 01:32:12] Creando DIM_HOGAR...\n",
      "[2025-11-04 01:32:12]   ‚úì 94921 registros creados\n",
      "[2025-11-04 01:32:12] Creando DIM_MADRE...\n",
      "[2025-11-04 01:32:12]   ‚úì 99928 registros creados\n",
      "[2025-11-04 01:32:12] Creando DIM_NINO...\n",
      "[2025-11-04 01:32:12]   ‚úì 131470 registros creados\n",
      "[2025-11-04 01:32:12] \n",
      "3. Creando tabla de hechos...\n",
      "[2025-11-04 01:32:12] Creando FACT_ANEMIA...\n",
      "[2025-11-04 01:32:12]   ‚úì 189846 registros creados\n",
      "[2025-11-04 01:32:12] \n",
      "4. Validando modelo...\n",
      "[2025-11-04 01:32:12] \n",
      "Validando integridad referencial...\n",
      "[2025-11-04 01:32:12]   ‚úì id_tiempo: OK\n",
      "[2025-11-04 01:32:12]   ‚úì id_geografia: OK\n",
      "[2025-11-04 01:32:12]   ‚úì id_hogar: OK\n",
      "[2025-11-04 01:32:12]   ‚úì id_madre: OK\n",
      "[2025-11-04 01:32:12]   ‚úì id_nino: OK\n",
      "[2025-11-04 01:32:12]   ‚úÖ Integridad referencial OK\n",
      "[2025-11-04 01:32:12] \n",
      "Validando calidad de datos...\n",
      "[2025-11-04 01:32:12]   Total registros FACT: 189,846\n",
      "[2025-11-04 01:32:12]   ‚ö†Ô∏è  Duplicados encontrados: 54501\n",
      "[2025-11-04 01:32:12] \n",
      "Dimensiones:\n",
      "[2025-11-04 01:32:12]   DIM_TIEMPO:        10 registros\n",
      "[2025-11-04 01:32:12]   DIM_GEOGRAFIA:  8,090 registros\n",
      "[2025-11-04 01:32:12]   DIM_HOGAR:     94,921 registros\n",
      "[2025-11-04 01:32:12]   DIM_MADRE:     99,928 registros\n",
      "[2025-11-04 01:32:12]   DIM_NINO:      131,470 registros\n",
      "[2025-11-04 01:32:12] \n",
      "  ‚úÖ Validaci√≥n de calidad completada\n",
      "[2025-11-04 01:32:12] \n",
      "5. Cargando a Data Warehouse...\n",
      "[2025-11-04 01:32:12] \n",
      "Guardando en SQLite: D:\\Data_Warehouse\\anemia_dwh.db\n",
      "[2025-11-04 01:32:12] \n",
      "Creando esquema SQLite...\n",
      "[2025-11-04 01:32:12]   ‚úì Tablas anteriores eliminadas (si exist√≠an)\n",
      "[2025-11-04 01:32:12]   Guardando dimensiones...\n",
      "[2025-11-04 01:32:13]   Guardando tabla de hechos...\n",
      "[2025-11-04 01:32:14]   Creando √≠ndices...\n",
      "[2025-11-04 01:32:15]   ‚úÖ Guardado en SQLite completado\n",
      "[2025-11-04 01:32:15] \n",
      "Creando backup Parquet en D:\\Data_Warehouse\\backup...\n",
      "[2025-11-04 01:32:16]   ‚úÖ Backup Parquet completado\n",
      "[2025-11-04 01:32:16] \n",
      "Generando metadata...\n",
      "[2025-11-04 01:32:16]   ‚úì Metadata guardada\n",
      "[2025-11-04 01:32:16] \n",
      "6. Verificaci√≥n final...\n",
      "[2025-11-04 01:32:16] \n",
      "üìä Casos por a√±o (verificaci√≥n):\n",
      "[2025-11-04 01:32:16]  anio  casos  con_anemia  prevalencia\n",
      " 2015  23561     11560.0        49.06\n",
      " 2016  20395     10113.0        49.59\n",
      " 2017  20669     10289.0        49.78\n",
      " 2018  10591     10591.0       100.00\n",
      " 2019  16095      5511.0        34.24\n",
      " 2020   9521      3223.0        33.85\n",
      " 2021  17100      6034.0        35.29\n",
      " 2022  15748      5770.0        36.64\n",
      " 2023  14210      5202.0        36.61\n",
      " 2024  13246      5049.0        38.12\n",
      "[2025-11-04 01:32:16] \n",
      "============================================================\n",
      "[2025-11-04 01:32:16] ‚úÖ MODELO ESTRELLA GENERADO EXITOSAMENTE\n",
      "[2025-11-04 01:32:16] ============================================================\n",
      "[2025-11-04 01:32:16] \n",
      "üìÅ Archivos generados:\n",
      "[2025-11-04 01:32:16]    ‚Ä¢ SQLite:  D:\\Data_Warehouse\\anemia_dwh.db\n",
      "[2025-11-04 01:32:16]    ‚Ä¢ Metadata: D:\\Data_Warehouse\\metadata\n",
      "[2025-11-04 01:32:16]    ‚Ä¢ Backup:   D:\\Data_Warehouse\\backup\n",
      "[2025-11-04 01:32:16] \n",
      "üìä Estad√≠sticas:\n",
      "[2025-11-04 01:32:16]    ‚Ä¢ Tama√±o DB: 44.27 MB\n",
      "[2025-11-04 01:32:16]    ‚Ä¢ Total registros FACT: 189,846\n",
      "[2025-11-04 01:32:16]    ‚Ä¢ Prevalencia anemia: 45.52%\n",
      "[2025-11-04 01:32:16]    ‚Ä¢ A√±os en dim_tiempo: 10\n",
      "[2025-11-04 01:32:16] \n",
      "============================================================\n",
      "[2025-11-04 01:32:16] Proceso completado en 4.89 segundos\n",
      "[2025-11-04 01:32:16] ============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GENERADOR DE MODELO ESTRELLA - ANEMIA INFANTIL ENDES (VERSI√ìN CORREGIDA)\n",
    "Convierte CSV consolidado ‚Üí SQLite con esquema estrella\n",
    "CORRECCI√ìN: dim_tiempo ahora tiene 1 registro por a√±o (no 12 meses)\n",
    "Autor: [Tu nombre]\n",
    "Fecha: 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "class Config:\n",
    "    # Rutas\n",
    "    CSV_INPUT = r\"D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\"\n",
    "    DB_OUTPUT = r\"D:\\Data_Warehouse\\anemia_dwh.db\"\n",
    "    METADATA_DIR = Path(r\"D:\\Data_Warehouse\\metadata\")\n",
    "    BACKUP_DIR = Path(r\"D:\\Data_Warehouse\\backup\")\n",
    "    \n",
    "    # Crear directorios si no existen\n",
    "    METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Configuraci√≥n de procesamiento\n",
    "    CHUNK_SIZE = 5000  # Para inserci√≥n en SQLite\n",
    "    BACKUP_PARQUET = True  # Crear backup en parquet\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================\n",
    "# UTILIDADES\n",
    "# ============================================================\n",
    "class Logger:\n",
    "    \"\"\"Logger simple para ETL\"\"\"\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "    def log(self, mensaje):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        msg = f\"[{timestamp}] {mensaje}\"\n",
    "        print(msg)\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(msg + '\\n')\n",
    "    \n",
    "    def finalizar(self):\n",
    "        duracion = (datetime.now() - self.start_time).total_seconds()\n",
    "        self.log(f\"\\n{'='*60}\")\n",
    "        self.log(f\"Proceso completado en {duracion:.2f} segundos\")\n",
    "        self.log(f\"{'='*60}\")\n",
    "\n",
    "# Inicializar logger\n",
    "log_file = config.METADATA_DIR / f\"log_etl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logger = Logger(log_file)\n",
    "\n",
    "# ============================================================\n",
    "# FUNCIONES DE DIMENSIONES\n",
    "# ============================================================\n",
    "\n",
    "def crear_dim_tiempo(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n TIEMPO - CORREGIDA\n",
    "    Granularidad: 1 registro por A√ëO (no por mes)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_TIEMPO...\")\n",
    "    \n",
    "    anios = sorted(df['ANIO'].dropna().unique())\n",
    "    \n",
    "    dim_tiempo = []\n",
    "    for anio in anios:\n",
    "        anio_int = int(anio)\n",
    "        dim_tiempo.append({\n",
    "            'id_tiempo': anio_int * 100 + 6,  # Formato: 201506 (junio representativo)\n",
    "            'anio': anio_int,\n",
    "            'mes': 6,  # Mes representativo (junio - mitad del a√±o)\n",
    "            'trimestre': 2,  # Q2\n",
    "            'semestre': 1,   # S1\n",
    "            'nombre_mes': 'Junio',\n",
    "            'quinquenio': f\"{(anio_int//5)*5}-{(anio_int//5)*5+4}\",\n",
    "            'periodo_covid': int(anio_int in [2020, 2021])\n",
    "        })\n",
    "    \n",
    "    df_tiempo = pd.DataFrame(dim_tiempo)\n",
    "    logger.log(f\"  ‚úì {len(df_tiempo)} registros creados (1 por a√±o)\")\n",
    "    return df_tiempo\n",
    "\n",
    "\n",
    "def crear_dim_geografia(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n GEOGRAF√çA\n",
    "    Combina: departamento + √°rea + altitud\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_GEOGRAFIA...\")\n",
    "    \n",
    "    # Seleccionar columnas geogr√°ficas\n",
    "    geo_cols = ['HV024', 'HV025', 'HV040']\n",
    "    \n",
    "    # Verificar que existan\n",
    "    geo_cols_disponibles = [col for col in geo_cols if col in df.columns]\n",
    "    \n",
    "    if not geo_cols_disponibles:\n",
    "        logger.log(\"  ‚ö†Ô∏è No se encontraron columnas geogr√°ficas\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combinaciones √∫nicas\n",
    "    df_geo = df[geo_cols_disponibles].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_geo['id_geografia'] = range(1, len(df_geo) + 1)\n",
    "    \n",
    "    # Mapeo regi√≥n natural (ajustar seg√∫n tus datos reales)\n",
    "    mapa_region = {\n",
    "        'tumbes': 'Costa', 'piura': 'Costa', 'lambayeque': 'Costa', \n",
    "        'la libertad': 'Costa', 'ancash': 'Costa', 'lima': 'Costa',\n",
    "        'ica': 'Costa', 'arequipa': 'Costa', 'moquegua': 'Costa', 'tacna': 'Costa',\n",
    "        'callao': 'Costa', 'lima metropolitana': 'Costa',\n",
    "        'cajamarca': 'Sierra', 'huanuco': 'Sierra', 'pasco': 'Sierra',\n",
    "        'junin': 'Sierra', 'huancavelica': 'Sierra', 'ayacucho': 'Sierra',\n",
    "        'apurimac': 'Sierra', 'cusco': 'Sierra', 'puno': 'Sierra',\n",
    "        'loreto': 'Selva', 'amazonas': 'Selva', 'san martin': 'Selva',\n",
    "        'ucayali': 'Selva', 'madre de dios': 'Selva'\n",
    "    }\n",
    "    \n",
    "    if 'HV024' in df_geo.columns:\n",
    "        df_geo['region_natural'] = df_geo['HV024'].str.lower().str.strip().map(mapa_region)\n",
    "        df_geo['region_natural'] = df_geo['region_natural'].fillna('Otro')\n",
    "    \n",
    "    # Categorizar altitud\n",
    "    if 'HV040' in df_geo.columns:\n",
    "        df_geo['rango_altitud'] = pd.cut(\n",
    "            df_geo['HV040'],\n",
    "            bins=[-1, 500, 1500, 2500, 5000],\n",
    "            labels=['<500m', '500-1500m', '1500-2500m', '>2500m']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HV024': 'departamento',\n",
    "        'HV025': 'area_residencia',\n",
    "        'HV040': 'altitud_msnm'\n",
    "    }\n",
    "    df_geo.rename(columns={k: v for k, v in rename_map.items() if k in df_geo.columns}, \n",
    "                  inplace=True)\n",
    "    \n",
    "    # Ordenar columnas\n",
    "    columnas_ordenadas = ['id_geografia', 'departamento', 'region_natural', \n",
    "                          'area_residencia', 'altitud_msnm', 'rango_altitud']\n",
    "    columnas_disponibles = [col for col in columnas_ordenadas if col in df_geo.columns]\n",
    "    df_geo = df_geo[columnas_disponibles]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_geo)} registros creados\")\n",
    "    return df_geo\n",
    "\n",
    "\n",
    "def crear_dim_hogar(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n HOGAR\n",
    "    Granularidad: HHID √∫nico\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_HOGAR...\")\n",
    "    \n",
    "    # Columnas de hogar\n",
    "    hogar_cols = ['HHID', 'HV009', 'HV271', 'V190', \n",
    "                  'HV206', 'HV201', 'HV205', 'HV237']\n",
    "    \n",
    "    # Verificar columnas disponibles\n",
    "    hogar_cols_disponibles = [col for col in hogar_cols if col in df.columns]\n",
    "    \n",
    "    if 'HHID' not in hogar_cols_disponibles:\n",
    "        logger.log(\"  ‚ö†Ô∏è Columna HHID no encontrada\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extraer hogares √∫nicos\n",
    "    df_hogar = df[hogar_cols_disponibles].drop_duplicates(subset='HHID').reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_hogar['id_hogar'] = range(1, len(df_hogar) + 1)\n",
    "    \n",
    "    # Crear categor√≠as si las columnas existen\n",
    "    if 'HV009' in df_hogar.columns:\n",
    "        df_hogar['categoria_tamano'] = pd.cut(\n",
    "            df_hogar['HV009'],\n",
    "            bins=[0, 3, 5, 100],\n",
    "            labels=['Peque√±o', 'Mediano', 'Grande']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HV009': 'num_miembros',\n",
    "        'HV271': 'indice_riqueza_num',\n",
    "        'V190': 'quintil_riqueza',\n",
    "        'HV206': 'tiene_electricidad',\n",
    "        'HV201': 'fuente_agua',\n",
    "        'HV205': 'tipo_saneamiento',\n",
    "        'HV237': 'trata_agua'\n",
    "    }\n",
    "    \n",
    "    df_hogar.rename(columns={k: v for k, v in rename_map.items() if k in df_hogar.columns}, \n",
    "                    inplace=True)\n",
    "    \n",
    "    # Mover id_hogar y HHID al inicio\n",
    "    cols = ['id_hogar', 'HHID'] + [col for col in df_hogar.columns if col not in ['id_hogar', 'HHID']]\n",
    "    df_hogar = df_hogar[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_hogar)} registros creados\")\n",
    "    return df_hogar\n",
    "\n",
    "\n",
    "def crear_dim_madre(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n MADRE\n",
    "    Granularidad: CASEID √∫nico\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_MADRE...\")\n",
    "    \n",
    "    # Columnas de madre\n",
    "    madre_cols = ['CASEID', 'V012', 'V106', 'V133', 'V025']\n",
    "    madre_cols_disponibles = [col for col in madre_cols if col in df.columns]\n",
    "    \n",
    "    if 'CASEID' not in madre_cols_disponibles:\n",
    "        logger.log(\"  ‚ö†Ô∏è Columna CASEID no encontrada\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extraer madres √∫nicas\n",
    "    df_madre = df[madre_cols_disponibles].drop_duplicates(subset='CASEID').reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_madre['id_madre'] = range(1, len(df_madre) + 1)\n",
    "    \n",
    "    # Crear categor√≠as si existen las columnas\n",
    "    if 'V012' in df_madre.columns:\n",
    "        df_madre['rango_edad'] = pd.cut(\n",
    "            df_madre['V012'],\n",
    "            bins=[0, 20, 35, 100],\n",
    "            labels=['<20', '20-34', '35+']\n",
    "        ).astype(str)\n",
    "        \n",
    "        df_madre['es_madre_adolescente'] = (df_madre['V012'] < 20).astype(int)\n",
    "    \n",
    "    if 'V133' in df_madre.columns:\n",
    "        df_madre['categoria_educacion'] = pd.cut(\n",
    "            df_madre['V133'],\n",
    "            bins=[-1, 6, 12, 100],\n",
    "            labels=['Baja', 'Media', 'Alta']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar\n",
    "    rename_map = {\n",
    "        'V012': 'edad_actual',\n",
    "        'V106': 'nivel_educativo',\n",
    "        'V133': 'anios_educacion',\n",
    "        'V025': 'area_residencia_madre'\n",
    "    }\n",
    "    \n",
    "    df_madre.rename(columns={k: v for k, v in rename_map.items() if k in df_madre.columns}, \n",
    "                    inplace=True)\n",
    "    \n",
    "    # Reordenar\n",
    "    cols = ['id_madre', 'CASEID'] + [col for col in df_madre.columns if col not in ['id_madre', 'CASEID']]\n",
    "    df_madre = df_madre[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_madre)} registros creados\")\n",
    "    return df_madre\n",
    "\n",
    "\n",
    "def crear_dim_nino(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n NI√ëO\n",
    "    Granularidad: HHID + HC0 (cada ni√±o)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_NINO...\")\n",
    "    \n",
    "    # Cada registro es un ni√±o √∫nico\n",
    "    nino_cols = ['HHID', 'HC0', 'HC1', 'HC27', 'BORD', 'HC70', 'HW71']\n",
    "    nino_cols_disponibles = [col for col in nino_cols if col in df.columns]\n",
    "    \n",
    "    if 'HHID' not in nino_cols_disponibles or 'HC0' not in nino_cols_disponibles:\n",
    "        logger.log(\"  ‚ö†Ô∏è Columnas HHID o HC0 no encontradas\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df_nino = df[nino_cols_disponibles].copy()\n",
    "    \n",
    "    # Crear ID √∫nico\n",
    "    df_nino['id_nino'] = range(1, len(df_nino) + 1)\n",
    "    \n",
    "    # Crear categor√≠as\n",
    "    if 'HC1' in df_nino.columns:\n",
    "        df_nino['rango_edad'] = pd.cut(\n",
    "            df_nino['HC1'],\n",
    "            bins=[5, 12, 18, 24, 36],\n",
    "            labels=['6-11m', '12-17m', '18-23m', '24-35m'],\n",
    "            right=False\n",
    "        ).astype(str)\n",
    "    \n",
    "    if 'HC70' in df_nino.columns:\n",
    "        df_nino['tiene_desnutricion_cronica'] = (df_nino['HC70'] < -2).astype(int)\n",
    "    \n",
    "    if 'HW71' in df_nino.columns:\n",
    "        df_nino['tiene_bajo_peso'] = (df_nino['HW71'] < -2).astype(int)\n",
    "    \n",
    "    if 'BORD' in df_nino.columns:\n",
    "        df_nino['categoria_orden'] = pd.cut(\n",
    "            df_nino['BORD'],\n",
    "            bins=[0, 1, 3, 100],\n",
    "            labels=['Primog√©nito', '2-3', '4+']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar\n",
    "    rename_map = {\n",
    "        'HC1': 'edad_meses',\n",
    "        'HC27': 'sexo',\n",
    "        'BORD': 'orden_nacimiento'\n",
    "    }\n",
    "    \n",
    "    df_nino.rename(columns={k: v for k, v in rename_map.items() if k in df_nino.columns}, \n",
    "                   inplace=True)\n",
    "    \n",
    "    # Reordenar\n",
    "    cols = ['id_nino', 'HHID', 'HC0'] + [col for col in df_nino.columns \n",
    "                                          if col not in ['id_nino', 'HHID', 'HC0']]\n",
    "    df_nino = df_nino[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_nino)} registros creados\")\n",
    "    return df_nino\n",
    "\n",
    "\n",
    "def crear_fact_anemia(df, dim_tiempo, dim_geo, dim_hogar, dim_madre, dim_nino):\n",
    "    \"\"\"\n",
    "    Tabla de HECHOS - FACT_ANEMIA\n",
    "    CORREGIDO: Mapeo 1:1 con dim_tiempo (1 registro por a√±o)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando FACT_ANEMIA...\")\n",
    "    \n",
    "    fact = df.copy()\n",
    "    \n",
    "    # 1. Mapear FK - TIEMPO (CORREGIDO: asignar a junio de cada a√±o)\n",
    "    if 'ANIO' in fact.columns:\n",
    "        fact['id_tiempo'] = fact['ANIO'].astype(int) * 100 + 6\n",
    "    else:\n",
    "        logger.log(\"  ‚ö†Ô∏è Columna ANIO no encontrada\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 2. Mapear FK - GEOGRAFIA\n",
    "    if not dim_geo.empty and 'HV024' in fact.columns:\n",
    "        merge_cols = [col for col in ['HV024', 'HV025', 'HV040'] if col in fact.columns]\n",
    "        dim_merge_cols = [col.replace('HV024', 'departamento')\n",
    "                          .replace('HV025', 'area_residencia')\n",
    "                          .replace('HV040', 'altitud_msnm') for col in merge_cols]\n",
    "        \n",
    "        fact = fact.merge(\n",
    "            dim_geo[['id_geografia'] + dim_merge_cols],\n",
    "            left_on=merge_cols,\n",
    "            right_on=dim_merge_cols,\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Eliminar columnas duplicadas del merge\n",
    "        for col in dim_merge_cols:\n",
    "            if col in fact.columns and col not in ['id_geografia']:\n",
    "                fact.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # 3. Mapear FK - HOGAR\n",
    "    if not dim_hogar.empty and 'HHID' in fact.columns:\n",
    "        fact = fact.merge(\n",
    "            dim_hogar[['id_hogar', 'HHID']],\n",
    "            on='HHID',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    # 4. Mapear FK - MADRE\n",
    "    if not dim_madre.empty and 'CASEID' in fact.columns:\n",
    "        fact = fact.merge(\n",
    "            dim_madre[['id_madre', 'CASEID']],\n",
    "            on='CASEID',\n",
    "            how='left'\n",
    "        )\n",
    "    \n",
    "    # 5. Mapear FK - NINO\n",
    "    if not dim_nino.empty and 'HHID' in fact.columns and 'HC0' in fact.columns:\n",
    "        fact = fact.merge(\n",
    "            dim_nino[['id_nino', 'HHID', 'HC0']],\n",
    "            on=['HHID', 'HC0'],\n",
    "            how='left',\n",
    "            suffixes=('', '_nino')\n",
    "        )\n",
    "    \n",
    "    # 6. Seleccionar columnas de FACT\n",
    "    fact_cols = [\n",
    "        # FK\n",
    "        'id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre',\n",
    "        \n",
    "        # IDs originales (trazabilidad)\n",
    "        'HHID', 'CASEID', 'HC0',\n",
    "        \n",
    "        # M√©tricas antropom√©tricas\n",
    "        'HW2', 'HW3', 'HC70', 'HW70', 'HW71', 'HW72', 'HW73',\n",
    "        \n",
    "        # Target\n",
    "        'ANEMIA', 'HC57',\n",
    "        \n",
    "        # Pesos\n",
    "        'PESO',\n",
    "        \n",
    "        # Flags de calidad\n",
    "        'HC55', 'HV015', 'HV103'\n",
    "    ]\n",
    "    \n",
    "    # Filtrar solo columnas existentes\n",
    "    fact_cols_disponibles = [col for col in fact_cols if col in fact.columns]\n",
    "    fact = fact[fact_cols_disponibles]\n",
    "    \n",
    "    # 7. Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HW2': 'peso_kg',\n",
    "        'HW3': 'talla_cm',\n",
    "        'HC70': 'z_talla_edad',\n",
    "        'HW70': 'z_talla_edad_alt',\n",
    "        'HW71': 'z_peso_edad',\n",
    "        'HW72': 'z_peso_talla',\n",
    "        'HW73': 'z_imc',\n",
    "        'ANEMIA': 'tiene_anemia',\n",
    "        'HC57': 'nivel_anemia',\n",
    "        'PESO': 'peso_muestral',\n",
    "        'HC55': 'medicion_valida',\n",
    "        'HV015': 'cuestionario_ok',\n",
    "        'HV103': 'durmio_anoche'\n",
    "    }\n",
    "    \n",
    "    fact.rename(columns={k: v for k, v in rename_map.items() if k in fact.columns}, \n",
    "                inplace=True)\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(fact)} registros creados\")\n",
    "    \n",
    "    return fact\n",
    "\n",
    "# ============================================================\n",
    "# VALIDACIONES\n",
    "# ============================================================\n",
    "\n",
    "def validar_integridad_referencial(fact, dims):\n",
    "    \"\"\"Validar FK en tabla de hechos\"\"\"\n",
    "    logger.log(\"\\nValidando integridad referencial...\")\n",
    "    \n",
    "    errores = []\n",
    "    \n",
    "    # Verificar cada FK\n",
    "    checks = [\n",
    "        ('id_tiempo', dims['dim_tiempo'], 'id_tiempo'),\n",
    "        ('id_geografia', dims['dim_geografia'], 'id_geografia'),\n",
    "        ('id_hogar', dims['dim_hogar'], 'id_hogar'),\n",
    "        ('id_madre', dims['dim_madre'], 'id_madre'),\n",
    "        ('id_nino', dims['dim_nino'], 'id_nino')\n",
    "    ]\n",
    "    \n",
    "    for fk_col, dim_df, dim_pk in checks:\n",
    "        if fk_col in fact.columns and not dim_df.empty:\n",
    "            invalidos = ~fact[fk_col].isin(dim_df[dim_pk])\n",
    "            n_invalidos = invalidos.sum()\n",
    "            \n",
    "            if n_invalidos > 0:\n",
    "                errores.append(f\"  ‚ùå {fk_col}: {n_invalidos} FK inv√°lidos\")\n",
    "            else:\n",
    "                logger.log(f\"  ‚úì {fk_col}: OK\")\n",
    "    \n",
    "    if errores:\n",
    "        logger.log(\"\\n‚ö†Ô∏è  ERRORES DE INTEGRIDAD ENCONTRADOS:\")\n",
    "        for error in errores:\n",
    "            logger.log(error)\n",
    "        raise ValueError(\"Integridad referencial violada\")\n",
    "    else:\n",
    "        logger.log(\"  ‚úÖ Integridad referencial OK\")\n",
    "\n",
    "\n",
    "def validar_calidad_datos(fact, dims):\n",
    "    \"\"\"Validar calidad de datos\"\"\"\n",
    "    logger.log(\"\\nValidando calidad de datos...\")\n",
    "    \n",
    "    # 1. Registros en fact\n",
    "    logger.log(f\"  Total registros FACT: {len(fact):,}\")\n",
    "    \n",
    "    # 2. Duplicados\n",
    "    if 'id_nino' in fact.columns and 'id_tiempo' in fact.columns:\n",
    "        duplicados = fact.duplicated(subset=['id_nino', 'id_tiempo']).sum()\n",
    "        if duplicados > 0:\n",
    "            logger.log(f\"  ‚ö†Ô∏è  Duplicados encontrados: {duplicados}\")\n",
    "        else:\n",
    "            logger.log(f\"  ‚úì Sin duplicados\")\n",
    "    \n",
    "    # 3. Missings en FK\n",
    "    for col in ['id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre']:\n",
    "        if col in fact.columns:\n",
    "            missing = fact[col].isna().sum()\n",
    "            if missing > 0:\n",
    "                logger.log(f\"  ‚ö†Ô∏è  {col}: {missing} missings ({missing/len(fact)*100:.1f}%)\")\n",
    "    \n",
    "    # 4. Estad√≠sticas de dimensiones\n",
    "    logger.log(f\"\\nDimensiones:\")\n",
    "    logger.log(f\"  DIM_TIEMPO:    {len(dims['dim_tiempo']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_GEOGRAFIA: {len(dims['dim_geografia']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_HOGAR:     {len(dims['dim_hogar']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_MADRE:     {len(dims['dim_madre']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_NINO:      {len(dims['dim_nino']):>6,} registros\")\n",
    "    \n",
    "    logger.log(\"\\n  ‚úÖ Validaci√≥n de calidad completada\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR EN SQLITE\n",
    "# ============================================================\n",
    "\n",
    "def crear_esquema_sqlite(conn):\n",
    "    \"\"\"Crear estructura de tablas con DDL\"\"\"\n",
    "    logger.log(\"\\nCreando esquema SQLite...\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # DDL para cada tabla (simplificado - SQLite infiere tipos)\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS fact_anemia\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_tiempo\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_geografia\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_hogar\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_madre\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_nino\")\n",
    "    \n",
    "    conn.commit()\n",
    "    logger.log(\"  ‚úì Tablas anteriores eliminadas (si exist√≠an)\")\n",
    "\n",
    "\n",
    "def guardar_en_sqlite(fact, dims, db_path):\n",
    "    \"\"\"Guardar modelo estrella en SQLite\"\"\"\n",
    "    logger.log(f\"\\nGuardando en SQLite: {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Crear esquema\n",
    "    crear_esquema_sqlite(conn)\n",
    "    \n",
    "    # Guardar dimensiones\n",
    "    logger.log(\"  Guardando dimensiones...\")\n",
    "    dims['dim_tiempo'].to_sql('dim_tiempo', conn, if_exists='replace', index=False)\n",
    "    dims['dim_geografia'].to_sql('dim_geografia', conn, if_exists='replace', index=False)\n",
    "    dims['dim_hogar'].to_sql('dim_hogar', conn, if_exists='replace', index=False)\n",
    "    dims['dim_madre'].to_sql('dim_madre', conn, if_exists='replace', index=False)\n",
    "    dims['dim_nino'].to_sql('dim_nino', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Guardar hechos (en chunks)\n",
    "    logger.log(\"  Guardando tabla de hechos...\")\n",
    "    fact.to_sql('fact_anemia', conn, if_exists='replace', \n",
    "                index=False, chunksize=config.CHUNK_SIZE)\n",
    "    \n",
    "    # Crear √≠ndices\n",
    "    logger.log(\"  Creando √≠ndices...\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    indices = [\n",
    "        \"CREATE INDEX idx_fact_tiempo ON fact_anemia(id_tiempo)\",\n",
    "        \"CREATE INDEX idx_fact_geografia ON fact_anemia(id_geografia)\",\n",
    "        \"CREATE INDEX idx_fact_hogar ON fact_anemia(id_hogar)\",\n",
    "        \"CREATE INDEX idx_fact_madre ON fact_anemia(id_madre)\",\n",
    "        \"CREATE INDEX idx_fact_nino ON fact_anemia(id_nino)\",\n",
    "        \"CREATE INDEX idx_fact_anemia ON fact_anemia(tiene_anemia)\",\n",
    "        \"CREATE INDEX idx_dim_geo_dept ON dim_geografia(departamento)\",\n",
    "        \"CREATE INDEX idx_dim_tiempo_anio ON dim_tiempo(anio)\"\n",
    "    ]\n",
    "    \n",
    "    for idx_sql in indices:\n",
    "        try:\n",
    "            cursor.execute(idx_sql)\n",
    "        except Exception as e:\n",
    "            logger.log(f\"    ‚ö†Ô∏è  Error creando √≠ndice: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logger.log(\"  ‚úÖ Guardado en SQLite completado\")\n",
    "\n",
    "\n",
    "def guardar_backup_parquet(fact, dims):\n",
    "    \"\"\"Backup en Parquet\"\"\"\n",
    "    if not config.BACKUP_PARQUET:\n",
    "        return\n",
    "    \n",
    "    logger.log(f\"\\nCreando backup Parquet en {config.BACKUP_DIR}...\")\n",
    "    \n",
    "    fact.to_parquet(config.BACKUP_DIR / \"fact_anemia.parquet\", index=False)\n",
    "    dims['dim_tiempo'].to_parquet(config.BACKUP_DIR / \"dim_tiempo.parquet\", index=False)\n",
    "    dims['dim_geografia'].to_parquet(config.BACKUP_DIR / \"dim_geografia.parquet\", index=False)\n",
    "    dims['dim_hogar'].to_parquet(config.BACKUP_DIR / \"dim_hogar.parquet\", index=False)\n",
    "    dims['dim_madre'].to_parquet(config.BACKUP_DIR / \"dim_madre.parquet\", index=False)\n",
    "    dims['dim_nino'].to_parquet(config.BACKUP_DIR / \"dim_nino.parquet\", index=False)\n",
    "    \n",
    "    logger.log(\"  ‚úÖ Backup Parquet completado\")\n",
    "\n",
    "\n",
    "def generar_metadata(fact, dims):\n",
    "    \"\"\"Generar metadata del warehouse\"\"\"\n",
    "    logger.log(\"\\nGenerando metadata...\")\n",
    "    \n",
    "    metadata = {\n",
    "        'fecha_creacion': datetime.now().isoformat(),\n",
    "        'version': 'v2.0_corregida',\n",
    "        'correcciones': 'dim_tiempo con 1 registro por a√±o (no 12 meses)',\n",
    "        'registros': {\n",
    "            'fact_anemia': len(fact),\n",
    "            'dim_tiempo': len(dims['dim_tiempo']),\n",
    "            'dim_geografia': len(dims['dim_geografia']),\n",
    "            'dim_hogar': len(dims['dim_hogar']),\n",
    "            'dim_madre': len(dims['dim_madre']),\n",
    "            'dim_nino': len(dims['dim_nino'])\n",
    "        },\n",
    "        'columnas': {\n",
    "            'fact_anemia': list(fact.columns),\n",
    "            'dim_tiempo': list(dims['dim_tiempo'].columns),\n",
    "            'dim_geografia': list(dims['dim_geografia'].columns),\n",
    "            'dim_hogar': list(dims['dim_hogar'].columns),\n",
    "            'dim_madre': list(dims['dim_madre'].columns),\n",
    "            'dim_nino': list(dims['dim_nino'].columns)\n",
    "        },\n",
    "        'prevalencia_anemia': float(fact['tiene_anemia'].mean() * 100) if 'tiene_anemia' in fact.columns else None\n",
    "    }\n",
    "    \n",
    "    # Guardar como JSON\n",
    "    with open(config.METADATA_DIR / 'estadisticas.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logger.log(\"  ‚úì Metadata guardada\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline completo ETL\"\"\"\n",
    "    \n",
    "    logger.log(\"=\"*60)\n",
    "    logger.log(\"INICIANDO GENERACI√ìN DE MODELO ESTRELLA (VERSI√ìN CORREGIDA)\")\n",
    "    logger.log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. EXTRACT\n",
    "        logger.log(f\"\\n1. Cargando CSV: {config.CSV_INPUT}\")\n",
    "        df = pd.read_csv(config.CSV_INPUT, encoding='utf-8-sig')\n",
    "        logger.log(f\"   ‚úì {len(df):,} registros cargados\")\n",
    "        logger.log(f\"   ‚úì {df.shape[1]} columnas\")\n",
    "        \n",
    "        # 2. TRANSFORM - Crear dimensiones\n",
    "        logger.log(\"\\n2. Creando dimensiones...\")\n",
    "        dim_tiempo = crear_dim_tiempo(df)\n",
    "        dim_geografia = crear_dim_geografia(df)\n",
    "        dim_hogar = crear_dim_hogar(df)\n",
    "        dim_madre = crear_dim_madre(df)\n",
    "        dim_nino = crear_dim_nino(df)\n",
    "        \n",
    "        dims = {\n",
    "            'dim_tiempo': dim_tiempo,\n",
    "            'dim_geografia': dim_geografia,\n",
    "            'dim_hogar': dim_hogar,\n",
    "            'dim_madre': dim_madre,\n",
    "            'dim_nino': dim_nino\n",
    "        }\n",
    "        \n",
    "        # 3. TRANSFORM - Crear hechos\n",
    "        logger.log(\"\\n3. Creando tabla de hechos...\")\n",
    "        fact = crear_fact_anemia(df, dim_tiempo, dim_geografia, \n",
    "                                dim_hogar, dim_madre, dim_nino)\n",
    "        \n",
    "        # 4. VALIDACIONES\n",
    "        logger.log(\"\\n4. Validando modelo...\")\n",
    "        validar_integridad_referencial(fact, dims)\n",
    "        validar_calidad_datos(fact, dims)\n",
    "        \n",
    "        # 5. LOAD\n",
    "        logger.log(\"\\n5. Cargando a Data Warehouse...\")\n",
    "        guardar_en_sqlite(fact, dims, config.DB_OUTPUT)\n",
    "        guardar_backup_parquet(fact, dims)\n",
    "        generar_metadata(fact, dims)\n",
    "        \n",
    "        # 6. VERIFICACI√ìN FINAL\n",
    "        logger.log(\"\\n6. Verificaci√≥n final...\")\n",
    "        conn = sqlite3.connect(config.DB_OUTPUT)\n",
    "        query_verificacion = \"\"\"\n",
    "        SELECT \n",
    "            t.anio,\n",
    "            COUNT(*) as casos,\n",
    "            SUM(f.tiene_anemia) as con_anemia,\n",
    "            ROUND(AVG(f.tiene_anemia) * 100, 2) as prevalencia\n",
    "        FROM fact_anemia f\n",
    "        JOIN dim_tiempo t ON f.id_tiempo = t.id_tiempo\n",
    "        WHERE f.tiene_anemia IS NOT NULL\n",
    "        GROUP BY t.anio\n",
    "        ORDER BY t.anio\n",
    "        \"\"\"\n",
    "        df_verificacion = pd.read_sql(query_verificacion, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        logger.log(\"\\nüìä Casos por a√±o (verificaci√≥n):\")\n",
    "        logger.log(df_verificacion.to_string(index=False))\n",
    "        \n",
    "        # FIN\n",
    "        logger.log(\"\\n\" + \"=\"*60)\n",
    "        logger.log(\"‚úÖ MODELO ESTRELLA GENERADO EXITOSAMENTE\")\n",
    "        logger.log(\"=\"*60)\n",
    "        logger.log(f\"\\nüìÅ Archivos generados:\")\n",
    "        logger.log(f\"   ‚Ä¢ SQLite:  {config.DB_OUTPUT}\")\n",
    "        logger.log(f\"   ‚Ä¢ Metadata: {config.METADATA_DIR}\")\n",
    "        logger.log(f\"   ‚Ä¢ Backup:   {config.BACKUP_DIR}\")\n",
    "        \n",
    "        # Estad√≠sticas finales\n",
    "        tamanio_db = Path(config.DB_OUTPUT).stat().st_size / (1024*1024)\n",
    "        logger.log(f\"\\nüìä Estad√≠sticas:\")\n",
    "        logger.log(f\"   ‚Ä¢ Tama√±o DB: {tamanio_db:.2f} MB\")\n",
    "        logger.log(f\"   ‚Ä¢ Total registros FACT: {len(fact):,}\")\n",
    "        if 'tiene_anemia' in fact.columns:\n",
    "            logger.log(f\"   ‚Ä¢ Prevalencia anemia: {fact['tiene_anemia'].mean()*100:.2f}%\")\n",
    "        logger.log(f\"   ‚Ä¢ A√±os en dim_tiempo: {len(dim_tiempo)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.log(traceback.format_exc())\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        logger.finalizar()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257169d-8375-431c-b6e1-67e1d5149ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47cdc4e-0607-448f-87b5-f84ec7422aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d30d6-41d0-49f6-b406-0348c3cb7315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77855611-63e3-4f79-b830-57056a47839d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34ff378-bbf0-423b-a4a6-0593d507032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-04 02:07:44] ============================================================\n",
      "[2025-11-04 02:07:44] INICIANDO GENERACI√ìN DE MODELO ESTRELLA\n",
      "[2025-11-04 02:07:44] ============================================================\n",
      "[2025-11-04 02:07:44] \n",
      "1. Cargando CSV: D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\n",
      "[2025-11-04 02:07:44]    ‚úì 131,470 registros cargados\n",
      "[2025-11-04 02:07:44]    ‚úì 42 columnas\n",
      "[2025-11-04 02:07:44] \n",
      "2. Creando dimensiones...\n",
      "[2025-11-04 02:07:44] Creando DIM_TIEMPO (CORREGIDA - 1 registro por a√±o)...\n",
      "[2025-11-04 02:07:44]   ‚úì 10 registros creados (1 por a√±o)\n",
      "[2025-11-04 02:07:44] Creando DIM_GEOGRAFIA...\n",
      "[2025-11-04 02:07:45]   ‚úì 8090 registros creados\n",
      "[2025-11-04 02:07:45] Creando DIM_HOGAR...\n",
      "[2025-11-04 02:07:45]   ‚úì 94921 registros creados\n",
      "[2025-11-04 02:07:45] Creando DIM_MADRE...\n",
      "[2025-11-04 02:07:45]   ‚úì 99928 registros creados\n",
      "[2025-11-04 02:07:45] Creando DIM_NINO...\n",
      "[2025-11-04 02:07:45]   ‚ö†Ô∏è  Encontrados 26552 duplicados en HHID+HC0\n",
      "[2025-11-04 02:07:45]   ‚ö†Ô∏è  Total registros duplicados (incluyendo originales): 50674\n",
      "[2025-11-04 02:07:45]   ‚ö†Ô∏è  1540 grupos con datos conflictivos\n",
      "[2025-11-04 02:07:45]   ‚úì 104918 registros creados (duplicados eliminados)\n",
      "[2025-11-04 02:07:45] \n",
      "3. Creando tabla de hechos...\n",
      "[2025-11-04 02:07:45] Creando FACT_ANEMIA...\n",
      "[2025-11-04 02:07:45]   Eliminando duplicados del dataset original...\n",
      "[2025-11-04 02:07:45]   ‚ö†Ô∏è  Eliminados 24988 duplicados de HHID+HC0+ANIO\n",
      "[2025-11-04 02:07:45]   ‚ö†Ô∏è  Registros antes: 131,470, despu√©s: 106,482\n",
      "[2025-11-04 02:07:45]   Mapeando geograf√≠a...\n",
      "[2025-11-04 02:07:45]   Mapeando hogar...\n",
      "[2025-11-04 02:07:45]   Mapeando madre...\n",
      "[2025-11-04 02:07:45]   Mapeando ni√±o...\n",
      "[2025-11-04 02:07:45]   Verificando duplicados finales...\n",
      "[2025-11-04 02:07:45]   Verificando integridad de FK...\n",
      "[2025-11-04 02:07:45]   ‚úì 106482 registros creados (limpio de duplicados)\n",
      "[2025-11-04 02:07:45] \n",
      "4. Validando modelo...\n",
      "[2025-11-04 02:07:45] \n",
      "Validando integridad referencial...\n",
      "[2025-11-04 02:07:45]   ‚úì id_tiempo: OK\n",
      "[2025-11-04 02:07:45]   ‚úì id_geografia: OK\n",
      "[2025-11-04 02:07:45]   ‚úì id_hogar: OK\n",
      "[2025-11-04 02:07:45]   ‚úì id_madre: OK\n",
      "[2025-11-04 02:07:45]   ‚úì id_nino: OK\n",
      "[2025-11-04 02:07:45]   ‚úÖ Integridad referencial OK\n",
      "[2025-11-04 02:07:45] \n",
      "Validando calidad de datos...\n",
      "[2025-11-04 02:07:45]   Total registros FACT: 106,482\n",
      "[2025-11-04 02:07:45]   ‚úì Sin duplicados\n",
      "[2025-11-04 02:07:45] \n",
      "Dimensiones:\n",
      "[2025-11-04 02:07:45]   DIM_TIEMPO:        10 registros\n",
      "[2025-11-04 02:07:45]   DIM_GEOGRAFIA:  8,090 registros\n",
      "[2025-11-04 02:07:45]   DIM_HOGAR:     94,921 registros\n",
      "[2025-11-04 02:07:45]   DIM_MADRE:     99,928 registros\n",
      "[2025-11-04 02:07:45]   DIM_NINO:      104,918 registros\n",
      "[2025-11-04 02:07:45] \n",
      "  ‚úÖ Validaci√≥n de calidad completada\n",
      "[2025-11-04 02:07:45] \n",
      "5. Cargando a Data Warehouse...\n",
      "[2025-11-04 02:07:45] \n",
      "Guardando en SQLite: D:\\Data_Warehouse\\anemia_dwh.db\n",
      "[2025-11-04 02:07:45] \n",
      "Creando esquema SQLite...\n",
      "[2025-11-04 02:07:45]   ‚úì Tablas anteriores eliminadas (si exist√≠an)\n",
      "[2025-11-04 02:07:45]   Guardando dimensiones...\n",
      "[2025-11-04 02:07:46]   Guardando tabla de hechos...\n",
      "[2025-11-04 02:07:47]   Creando √≠ndices...\n",
      "[2025-11-04 02:07:47]   ‚úÖ Guardado en SQLite completado\n",
      "[2025-11-04 02:07:47] \n",
      "Creando backup Parquet en D:\\Data_Warehouse\\backup...\n",
      "[2025-11-04 02:07:48]   ‚úÖ Backup Parquet completado\n",
      "[2025-11-04 02:07:48] \n",
      "Generando metadata...\n",
      "[2025-11-04 02:07:48]   ‚úì Metadata guardada\n",
      "[2025-11-04 02:07:48] \n",
      "============================================================\n",
      "[2025-11-04 02:07:48] ‚úÖ MODELO ESTRELLA GENERADO EXITOSAMENTE\n",
      "[2025-11-04 02:07:48] ============================================================\n",
      "[2025-11-04 02:07:48] \n",
      "üìÅ Archivos generados:\n",
      "[2025-11-04 02:07:48]    ‚Ä¢ SQLite:  D:\\Data_Warehouse\\anemia_dwh.db\n",
      "[2025-11-04 02:07:48]    ‚Ä¢ Metadata: D:\\Data_Warehouse\\metadata\n",
      "[2025-11-04 02:07:48]    ‚Ä¢ Backup:   D:\\Data_Warehouse\\backup\n",
      "[2025-11-04 02:07:48] \n",
      "üìä Estad√≠sticas:\n",
      "[2025-11-04 02:07:48]    ‚Ä¢ Tama√±o DB: 44.27 MB\n",
      "[2025-11-04 02:07:48]    ‚Ä¢ Total registros FACT: 106,482\n",
      "[2025-11-04 02:07:48]    ‚Ä¢ Prevalencia anemia: 42.74%\n",
      "[2025-11-04 02:07:48] \n",
      "============================================================\n",
      "[2025-11-04 02:07:48] Proceso completado en 3.80 segundos\n",
      "[2025-11-04 02:07:48] ============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GENERADOR DE MODELO ESTRELLA - ANEMIA INFANTIL ENDES\n",
    "Convierte CSV consolidado ‚Üí SQLite con esquema estrella\n",
    "Autor: [Tu nombre]\n",
    "Fecha: 2025\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "class Config:\n",
    "    # Rutas\n",
    "    CSV_INPUT = r\"D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\"\n",
    "    DB_OUTPUT = r\"D:\\Data_Warehouse\\anemia_dwh.db\"\n",
    "    METADATA_DIR = Path(r\"D:\\Data_Warehouse\\metadata\")\n",
    "    BACKUP_DIR = Path(r\"D:\\Data_Warehouse\\backup\")\n",
    "    \n",
    "    # Crear directorios si no existen\n",
    "    METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Configuraci√≥n de procesamiento\n",
    "    CHUNK_SIZE = 5000  # Para inserci√≥n en SQLite\n",
    "    BACKUP_PARQUET = True  # Crear backup en parquet\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================\n",
    "# UTILIDADES\n",
    "# ============================================================\n",
    "class Logger:\n",
    "    \"\"\"Logger simple para ETL\"\"\"\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "    def log(self, mensaje):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        msg = f\"[{timestamp}] {mensaje}\"\n",
    "        print(msg)\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(msg + '\\n')\n",
    "    \n",
    "    def finalizar(self):\n",
    "        duracion = (datetime.now() - self.start_time).total_seconds()\n",
    "        self.log(f\"\\n{'='*60}\")\n",
    "        self.log(f\"Proceso completado en {duracion:.2f} segundos\")\n",
    "        self.log(f\"{'='*60}\")\n",
    "\n",
    "# Inicializar logger\n",
    "log_file = config.METADATA_DIR / f\"log_etl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logger = Logger(log_file)\n",
    "\n",
    "# ============================================================\n",
    "# FUNCIONES DE DIMENSIONES\n",
    "# ============================================================\n",
    "\n",
    "def crear_dim_tiempo(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n TIEMPO CORREGIDA\n",
    "    Granularidad: 1 registro por a√±o (simplificado para ENDES)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_TIEMPO (CORREGIDA - 1 registro por a√±o)...\")\n",
    "    \n",
    "    anios = sorted(df['ANIO'].unique())\n",
    "    \n",
    "    dim_tiempo = []\n",
    "    for anio in anios:\n",
    "        # Solo 1 registro por a√±o (usamos junio como representativo)\n",
    "        dim_tiempo.append({\n",
    "            'id_tiempo': int(f\"{anio}06\"),  # Formato: A√±o + Mes 06 (junio)\n",
    "            'anio': int(anio),\n",
    "            'mes': 6,  # Todos en junio\n",
    "            'trimestre': 2,  # Q2\n",
    "            'semestre': 1,   # S1\n",
    "            'nombre_mes': 'Junio',\n",
    "            'quinquenio': f\"{(anio//5)*5}-{(anio//5)*5+4}\",\n",
    "            'periodo_covid': int(anio in [2020, 2021])\n",
    "        })\n",
    "    \n",
    "    df_tiempo = pd.DataFrame(dim_tiempo)\n",
    "    logger.log(f\"  ‚úì {len(df_tiempo)} registros creados (1 por a√±o)\")\n",
    "    return df_tiempo\n",
    "\n",
    "\n",
    "def crear_dim_geografia(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n GEOGRAF√çA\n",
    "    Combina: departamento + √°rea + altitud\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_GEOGRAFIA...\")\n",
    "    \n",
    "    # Seleccionar columnas geogr√°ficas\n",
    "    geo_cols = ['HV024', 'HV025', 'HV040']\n",
    "    \n",
    "    # Combinaciones √∫nicas\n",
    "    df_geo = df[geo_cols].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_geo['id_geografia'] = range(1, len(df_geo) + 1)\n",
    "    \n",
    "    # Mapeo regi√≥n natural (simplificado - ajustar seg√∫n necesidad)\n",
    "    mapa_region = {\n",
    "        'tumbes': 'Costa', 'piura': 'Costa', 'lambayeque': 'Costa', \n",
    "        'la libertad': 'Costa', 'ancash': 'Costa', 'lima': 'Costa',\n",
    "        'ica': 'Costa', 'arequipa': 'Costa', 'moquegua': 'Costa', 'tacna': 'Costa',\n",
    "        'cajamarca': 'Sierra', 'huanuco': 'Sierra', 'pasco': 'Sierra',\n",
    "        'junin': 'Sierra', 'huancavelica': 'Sierra', 'ayacucho': 'Sierra',\n",
    "        'apurimac': 'Sierra', 'cusco': 'Sierra', 'puno': 'Sierra',\n",
    "        'loreto': 'Selva', 'amazonas': 'Selva', 'san martin': 'Selva',\n",
    "        'ucayali': 'Selva', 'madre de dios': 'Selva'\n",
    "    }\n",
    "    \n",
    "    df_geo['region_natural'] = df_geo['HV024'].str.lower().map(mapa_region)\n",
    "    df_geo['region_natural'] = df_geo['region_natural'].fillna('Otro')\n",
    "    \n",
    "    # Categorizar altitud\n",
    "    df_geo['rango_altitud'] = pd.cut(\n",
    "        df_geo['HV040'],\n",
    "        bins=[-1, 500, 1500, 2500, 5000],\n",
    "        labels=['<500m', '500-1500m', '1500-2500m', '>2500m']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    df_geo.rename(columns={\n",
    "        'HV024': 'departamento',\n",
    "        'HV025': 'area_residencia',\n",
    "        'HV040': 'altitud_msnm'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Ordenar columnas\n",
    "    df_geo = df_geo[['id_geografia', 'departamento', 'region_natural', \n",
    "                     'area_residencia', 'altitud_msnm', 'rango_altitud']]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_geo)} registros creados\")\n",
    "    return df_geo\n",
    "\n",
    "\n",
    "def crear_dim_hogar(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n HOGAR\n",
    "    Granularidad: HHID √∫nico\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_HOGAR...\")\n",
    "    \n",
    "    # Columnas de hogar\n",
    "    hogar_cols = ['HHID', 'HV009', 'HV271', 'V190', \n",
    "                  'HV206', 'HV201', 'HV205', 'HV237']\n",
    "    \n",
    "    # Verificar columnas disponibles\n",
    "    hogar_cols_disponibles = [col for col in hogar_cols if col in df.columns]\n",
    "    \n",
    "    # Extraer hogares √∫nicos\n",
    "    df_hogar = df[hogar_cols_disponibles].drop_duplicates(subset='HHID').reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_hogar['id_hogar'] = range(1, len(df_hogar) + 1)\n",
    "    \n",
    "    # Crear categor√≠as si las columnas existen\n",
    "    if 'HV009' in df_hogar.columns:\n",
    "        df_hogar['categoria_tamano'] = pd.cut(\n",
    "            df_hogar['HV009'],\n",
    "            bins=[0, 3, 5, 100],\n",
    "            labels=['Peque√±o', 'Mediano', 'Grande']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HV009': 'num_miembros',\n",
    "        'HV271': 'indice_riqueza_num',\n",
    "        'V190': 'quintil_riqueza',\n",
    "        'HV206': 'tiene_electricidad',\n",
    "        'HV201': 'fuente_agua',\n",
    "        'HV205': 'tipo_saneamiento',\n",
    "        'HV237': 'trata_agua'\n",
    "    }\n",
    "    \n",
    "    df_hogar.rename(columns={k: v for k, v in rename_map.items() if k in df_hogar.columns}, \n",
    "                    inplace=True)\n",
    "    \n",
    "    # Mover id_hogar y HHID al inicio\n",
    "    cols = ['id_hogar', 'HHID'] + [col for col in df_hogar.columns if col not in ['id_hogar', 'HHID']]\n",
    "    df_hogar = df_hogar[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_hogar)} registros creados\")\n",
    "    return df_hogar\n",
    "\n",
    "\n",
    "def crear_dim_madre(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n MADRE\n",
    "    Granularidad: CASEID √∫nico\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_MADRE...\")\n",
    "    \n",
    "    # Columnas de madre\n",
    "    madre_cols = ['CASEID', 'V012', 'V106', 'V133', 'V025']\n",
    "    madre_cols_disponibles = [col for col in madre_cols if col in df.columns]\n",
    "    \n",
    "    # Extraer madres √∫nicas\n",
    "    df_madre = df[madre_cols_disponibles].drop_duplicates(subset='CASEID').reset_index(drop=True)\n",
    "    \n",
    "    # Asignar ID\n",
    "    df_madre['id_madre'] = range(1, len(df_madre) + 1)\n",
    "    \n",
    "    # Crear categor√≠as si existen las columnas\n",
    "    if 'V012' in df_madre.columns:\n",
    "        df_madre['rango_edad'] = pd.cut(\n",
    "            df_madre['V012'],\n",
    "            bins=[0, 20, 35, 100],\n",
    "            labels=['<20', '20-34', '35+']\n",
    "        ).astype(str)\n",
    "        \n",
    "        df_madre['es_madre_adolescente'] = (df_madre['V012'] < 20).astype(int)\n",
    "    \n",
    "    if 'V133' in df_madre.columns:\n",
    "        df_madre['categoria_educacion'] = pd.cut(\n",
    "            df_madre['V133'],\n",
    "            bins=[-1, 6, 12, 100],\n",
    "            labels=['Baja', 'Media', 'Alta']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar\n",
    "    rename_map = {\n",
    "        'V012': 'edad_actual',\n",
    "        'V106': 'nivel_educativo',\n",
    "        'V133': 'anios_educacion',\n",
    "        'V025': 'area_residencia_madre'\n",
    "    }\n",
    "    \n",
    "    df_madre.rename(columns={k: v for k, v in rename_map.items() if k in df_madre.columns}, \n",
    "                    inplace=True)\n",
    "    \n",
    "    # Reordenar\n",
    "    cols = ['id_madre', 'CASEID'] + [col for col in df_madre.columns if col not in ['id_madre', 'CASEID']]\n",
    "    df_madre = df_madre[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_madre)} registros creados\")\n",
    "    return df_madre\n",
    "\n",
    "\n",
    "def crear_dim_nino(df):\n",
    "    \"\"\"\n",
    "    Dimensi√≥n NI√ëO CORREGIDA (manejo de duplicados)\n",
    "    Granularidad: HHID + HC0 (cada ni√±o)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando DIM_NINO...\")\n",
    "    \n",
    "    # Cada registro es un ni√±o √∫nico\n",
    "    nino_cols = ['HHID', 'HC0', 'HC1', 'HC27', 'BORD', 'HC70', 'HW71']\n",
    "    nino_cols_disponibles = [col for col in nino_cols if col in df.columns]\n",
    "    \n",
    "    # INVESTIGAR DUPLICADOS\n",
    "    duplicados = df.duplicated(subset=['HHID', 'HC0']).sum()\n",
    "    if duplicados > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Encontrados {duplicados} duplicados en HHID+HC0\")\n",
    "        \n",
    "        # Analizar los duplicados\n",
    "        dup_info = df[df.duplicated(subset=['HHID', 'HC0'], keep=False)]\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Total registros duplicados (incluyendo originales): {len(dup_info)}\")\n",
    "        \n",
    "        # Ver diferencias en columnas clave\n",
    "        columnas_clave = ['HC1', 'HC27', 'BORD', 'ANEMIA']\n",
    "        columnas_disponibles = [col for col in columnas_clave if col in dup_info.columns]\n",
    "        \n",
    "        if columnas_disponibles:\n",
    "            dup_sample = dup_info.groupby(['HHID', 'HC0'])[columnas_disponibles].nunique()\n",
    "            conflictos = (dup_sample > 1).any(axis=1).sum()\n",
    "            logger.log(f\"  ‚ö†Ô∏è  {conflictos} grupos con datos conflictivos\")\n",
    "    \n",
    "    # ELIMINAR DUPLICADOS - mantener el primer registro\n",
    "    df_nino = df[nino_cols_disponibles].drop_duplicates(subset=['HHID', 'HC0'], keep='first').copy()\n",
    "    \n",
    "    # Crear ID √∫nico\n",
    "    df_nino['id_nino'] = range(1, len(df_nino) + 1)\n",
    "    \n",
    "    # Crear categor√≠as\n",
    "    if 'HC1' in df_nino.columns:\n",
    "        df_nino['rango_edad'] = pd.cut(\n",
    "            df_nino['HC1'],\n",
    "            bins=[5, 12, 18, 24, 36],\n",
    "            labels=['6-11m', '12-17m', '18-23m', '24-35m'],\n",
    "            right=False\n",
    "        ).astype(str)\n",
    "    \n",
    "    if 'HC70' in df_nino.columns:\n",
    "        df_nino['tiene_desnutricion_cronica'] = (df_nino['HC70'] < -2).astype(int)\n",
    "    \n",
    "    if 'HW71' in df_nino.columns:\n",
    "        df_nino['tiene_bajo_peso'] = (df_nino['HW71'] < -2).astype(int)\n",
    "    \n",
    "    if 'BORD' in df_nino.columns:\n",
    "        df_nino['categoria_orden'] = pd.cut(\n",
    "            df_nino['BORD'],\n",
    "            bins=[0, 1, 3, 100],\n",
    "            labels=['Primog√©nito', '2-3', '4+']\n",
    "        ).astype(str)\n",
    "    \n",
    "    # Renombrar\n",
    "    rename_map = {\n",
    "        'HC1': 'edad_meses',\n",
    "        'HC27': 'sexo',\n",
    "        'BORD': 'orden_nacimiento'\n",
    "    }\n",
    "    \n",
    "    df_nino.rename(columns={k: v for k, v in rename_map.items() if k in df_nino.columns}, \n",
    "                   inplace=True)\n",
    "    \n",
    "    # Reordenar\n",
    "    cols = ['id_nino', 'HHID', 'HC0'] + [col for col in df_nino.columns \n",
    "                                          if col not in ['id_nino', 'HHID', 'HC0']]\n",
    "    df_nino = df_nino[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_nino)} registros creados (duplicados eliminados)\")\n",
    "    return df_nino\n",
    "\n",
    "\n",
    "def crear_fact_anemia(df, dim_tiempo, dim_geo, dim_hogar, dim_madre, dim_nino):\n",
    "    \"\"\"\n",
    "    Tabla de HECHOS - FACT_ANEMIA (CORREGIDA - MANEJO DE DUPLICADOS)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando FACT_ANEMIA...\")\n",
    "    \n",
    "    fact = df.copy()\n",
    "    \n",
    "    # 1. Mapear FK - TIEMPO CORREGIDO\n",
    "    fact['id_tiempo'] = fact['ANIO'].astype(int) * 100 + 6\n",
    "    \n",
    "    # 2. ELIMINAR DUPLICADOS DEL DATASET ORIGINAL PRIMERO\n",
    "    logger.log(\"  Eliminando duplicados del dataset original...\")\n",
    "    registros_antes = len(fact)\n",
    "    fact = fact.drop_duplicates(subset=['HHID', 'HC0', 'ANIO'], keep='first')\n",
    "    registros_despues = len(fact)\n",
    "    \n",
    "    if registros_antes != registros_despues:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Eliminados {registros_antes - registros_despues} duplicados de HHID+HC0+ANIO\")\n",
    "    \n",
    "    # 3. Mapear FK - GEOGRAFIA\n",
    "    logger.log(\"  Mapeando geograf√≠a...\")\n",
    "    fact = fact.merge(\n",
    "        dim_geo[['id_geografia', 'departamento', 'area_residencia', 'altitud_msnm']],\n",
    "        left_on=['HV024', 'HV025', 'HV040'],\n",
    "        right_on=['departamento', 'area_residencia', 'altitud_msnm'],\n",
    "        how='left'\n",
    "    ).drop(columns=['departamento', 'area_residencia', 'altitud_msnm'])\n",
    "    \n",
    "    # 4. Mapear FK - HOGAR\n",
    "    logger.log(\"  Mapeando hogar...\")\n",
    "    fact = fact.merge(\n",
    "        dim_hogar[['id_hogar', 'HHID']],\n",
    "        on='HHID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 5. Mapear FK - MADRE\n",
    "    logger.log(\"  Mapeando madre...\")\n",
    "    fact = fact.merge(\n",
    "        dim_madre[['id_madre', 'CASEID']],\n",
    "        on='CASEID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Mapear FK - NINO\n",
    "    logger.log(\"  Mapeando ni√±o...\")\n",
    "    fact = fact.merge(\n",
    "        dim_nino[['id_nino', 'HHID', 'HC0']],\n",
    "        on=['HHID', 'HC0'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 7. Verificar duplicados finales\n",
    "    logger.log(\"  Verificando duplicados finales...\")\n",
    "    duplicados_finales = fact.duplicated(subset=['id_nino', 'id_tiempo']).sum()\n",
    "    if duplicados_finales > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Eliminando {duplicados_finales} duplicados finales...\")\n",
    "        fact = fact.drop_duplicates(subset=['id_nino', 'id_tiempo'], keep='first')\n",
    "    \n",
    "    # 8. Seleccionar columnas de FACT\n",
    "    fact_cols = [\n",
    "        # FK\n",
    "        'id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre',\n",
    "        \n",
    "        # IDs originales (trazabilidad)\n",
    "        'HHID', 'CASEID', 'HC0',\n",
    "        \n",
    "        # M√©tricas antropom√©tricas\n",
    "        'HW2', 'HW3', 'HC70', 'HW70', 'HW71', 'HW72', 'HW73',\n",
    "        \n",
    "        # Target\n",
    "        'ANEMIA', 'HC57',\n",
    "        \n",
    "        # Pesos\n",
    "        'PESO',\n",
    "        \n",
    "        # Flags de calidad\n",
    "        'HC55', 'HV015', 'HV103'\n",
    "    ]\n",
    "    \n",
    "    # Filtrar solo columnas existentes\n",
    "    fact_cols_disponibles = [col for col in fact_cols if col in fact.columns]\n",
    "    fact = fact[fact_cols_disponibles]\n",
    "    \n",
    "    # 9. Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HW2': 'peso_kg',\n",
    "        'HW3': 'talla_cm',\n",
    "        'HC70': 'z_talla_edad',\n",
    "        'HW70': 'z_talla_edad_alt',\n",
    "        'HW71': 'z_peso_edad',\n",
    "        'HW72': 'z_peso_talla',\n",
    "        'HW73': 'z_imc',\n",
    "        'ANEMIA': 'tiene_anemia',\n",
    "        'HC57': 'nivel_anemia',\n",
    "        'PESO': 'peso_muestral',\n",
    "        'HC55': 'medicion_valida',\n",
    "        'HV015': 'cuestionario_ok',\n",
    "        'HV103': 'durmio_anoche'\n",
    "    }\n",
    "    \n",
    "    fact.rename(columns={k: v for k, v in rename_map.items() if k in fact.columns}, \n",
    "                inplace=True)\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(fact)} registros creados (sin duplicados)\")\n",
    "    \n",
    "    return fact\n",
    "\n",
    "\n",
    "def crear_fact_anemia(df, dim_tiempo, dim_geo, dim_hogar, dim_madre, dim_nino):\n",
    "    \"\"\"\n",
    "    Tabla de HECHOS - FACT_ANEMIA (CORREGIDA - ELIMINACI√ìN COMPLETA DE DUPLICADOS)\n",
    "    \"\"\"\n",
    "    logger.log(\"Creando FACT_ANEMIA...\")\n",
    "    \n",
    "    # HACER UNA COPIA Y ELIMINAR DUPLICADOS INMEDIATAMENTE\n",
    "    fact = df.copy()\n",
    "    \n",
    "    # 1. ELIMINAR TODOS LOS DUPLICADOS DEL DATASET ORIGINAL PRIMERO\n",
    "    logger.log(\"  Eliminando duplicados del dataset original...\")\n",
    "    registros_antes = len(fact)\n",
    "    \n",
    "    # Identificar la clave √∫nica para cada ni√±o por a√±o\n",
    "    fact = fact.drop_duplicates(subset=['HHID', 'HC0', 'ANIO'], keep='first')\n",
    "    \n",
    "    registros_despues = len(fact)\n",
    "    duplicados_eliminados = registros_antes - registros_despues\n",
    "    \n",
    "    if duplicados_eliminados > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Eliminados {duplicados_eliminados} duplicados de HHID+HC0+ANIO\")\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Registros antes: {registros_antes:,}, despu√©s: {registros_despues:,}\")\n",
    "    \n",
    "    # 2. Mapear FK - TIEMPO CORREGIDO\n",
    "    fact['id_tiempo'] = fact['ANIO'].astype(int) * 100 + 6\n",
    "    \n",
    "    # 3. Mapear FK - GEOGRAFIA (sin validaci√≥n estricta)\n",
    "    logger.log(\"  Mapeando geograf√≠a...\")\n",
    "    fact = fact.merge(\n",
    "        dim_geo[['id_geografia', 'departamento', 'area_residencia', 'altitud_msnm']],\n",
    "        left_on=['HV024', 'HV025', 'HV040'],\n",
    "        right_on=['departamento', 'area_residencia', 'altitud_msnm'],\n",
    "        how='left'\n",
    "    ).drop(columns=['departamento', 'area_residencia', 'altitud_msnm'])\n",
    "    \n",
    "    # 4. Mapear FK - HOGAR (sin validaci√≥n estricta)\n",
    "    logger.log(\"  Mapeando hogar...\")\n",
    "    fact = fact.merge(\n",
    "        dim_hogar[['id_hogar', 'HHID']],\n",
    "        on='HHID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 5. Mapear FK - MADRE (sin validaci√≥n estricta)\n",
    "    logger.log(\"  Mapeando madre...\")\n",
    "    fact = fact.merge(\n",
    "        dim_madre[['id_madre', 'CASEID']],\n",
    "        on='CASEID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 6. Mapear FK - NINO (SIN VALIDACI√ìN - manejaremos duplicados despu√©s)\n",
    "    logger.log(\"  Mapeando ni√±o...\")\n",
    "    fact = fact.merge(\n",
    "        dim_nino[['id_nino', 'HHID', 'HC0']],\n",
    "        on=['HHID', 'HC0'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 7. VERIFICAR Y ELIMINAR CUALQUIER DUPLICADO RESTANTE\n",
    "    logger.log(\"  Verificando duplicados finales...\")\n",
    "    \n",
    "    # Verificar duplicados por la clave natural del hecho\n",
    "    duplicados_natural = fact.duplicated(subset=['HHID', 'HC0', 'id_tiempo']).sum()\n",
    "    if duplicados_natural > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Eliminando {duplicados_natural} duplicados naturales...\")\n",
    "        fact = fact.drop_duplicates(subset=['HHID', 'HC0', 'id_tiempo'], keep='first')\n",
    "    \n",
    "    # Verificar duplicados por la clave del hecho\n",
    "    duplicados_fk = fact.duplicated(subset=['id_nino', 'id_tiempo']).sum()\n",
    "    if duplicados_fk > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Eliminando {duplicados_fk} duplicados por FK...\")\n",
    "        fact = fact.drop_duplicates(subset=['id_nino', 'id_tiempo'], keep='first')\n",
    "    \n",
    "    # 8. VERIFICAR INTEGRIDAD DE LAS FK\n",
    "    logger.log(\"  Verificando integridad de FK...\")\n",
    "    \n",
    "    # Verificar si hay registros sin FK de ni√±o (esto indicar√≠a problemas serios)\n",
    "    sin_nino = fact['id_nino'].isna().sum()\n",
    "    if sin_nino > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  {sin_nino} registros sin FK de ni√±o\")\n",
    "        # Podemos eliminar estos registros o mantenerlos seg√∫n el caso\n",
    "        # fact = fact.dropna(subset=['id_nino'])\n",
    "    \n",
    "    # 9. Seleccionar columnas de FACT\n",
    "    fact_cols = [\n",
    "        # FK\n",
    "        'id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre',\n",
    "        \n",
    "        # IDs originales (trazabilidad)\n",
    "        'HHID', 'CASEID', 'HC0',\n",
    "        \n",
    "        # M√©tricas antropom√©tricas\n",
    "        'HW2', 'HW3', 'HC70', 'HW70', 'HW71', 'HW72', 'HW73',\n",
    "        \n",
    "        # Target\n",
    "        'ANEMIA', 'HC57',\n",
    "        \n",
    "        # Pesos\n",
    "        'PESO',\n",
    "        \n",
    "        # Flags de calidad\n",
    "        'HC55', 'HV015', 'HV103'\n",
    "    ]\n",
    "    \n",
    "    # Filtrar solo columnas existentes\n",
    "    fact_cols_disponibles = [col for col in fact_cols if col in fact.columns]\n",
    "    fact = fact[fact_cols_disponibles]\n",
    "    \n",
    "    # 10. Renombrar columnas\n",
    "    rename_map = {\n",
    "        'HW2': 'peso_kg',\n",
    "        'HW3': 'talla_cm',\n",
    "        'HC70': 'z_talla_edad',\n",
    "        'HW70': 'z_talla_edad_alt',\n",
    "        'HW71': 'z_peso_edad',\n",
    "        'HW72': 'z_peso_talla',\n",
    "        'HW73': 'z_imc',\n",
    "        'ANEMIA': 'tiene_anemia',\n",
    "        'HC57': 'nivel_anemia',\n",
    "        'PESO': 'peso_muestral',\n",
    "        'HC55': 'medicion_valida',\n",
    "        'HV015': 'cuestionario_ok',\n",
    "        'HV103': 'durmio_anoche'\n",
    "    }\n",
    "    \n",
    "    fact.rename(columns={k: v for k, v in rename_map.items() if k in fact.columns}, \n",
    "                inplace=True)\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(fact)} registros creados (limpio de duplicados)\")\n",
    "    \n",
    "    return fact\n",
    "\n",
    "# ============================================================\n",
    "# VALIDACIONES\n",
    "# ============================================================\n",
    "\n",
    "def validar_integridad_referencial(fact, dims):\n",
    "    \"\"\"Validar FK en tabla de hechos\"\"\"\n",
    "    logger.log(\"\\nValidando integridad referencial...\")\n",
    "    \n",
    "    errores = []\n",
    "    \n",
    "    # Verificar cada FK\n",
    "    checks = [\n",
    "        ('id_tiempo', dims['dim_tiempo'], 'id_tiempo'),\n",
    "        ('id_geografia', dims['dim_geografia'], 'id_geografia'),\n",
    "        ('id_hogar', dims['dim_hogar'], 'id_hogar'),\n",
    "        ('id_madre', dims['dim_madre'], 'id_madre'),\n",
    "        ('id_nino', dims['dim_nino'], 'id_nino')\n",
    "    ]\n",
    "    \n",
    "    for fk_col, dim_df, dim_pk in checks:\n",
    "        if fk_col in fact.columns:\n",
    "            invalidos = ~fact[fk_col].isin(dim_df[dim_pk])\n",
    "            n_invalidos = invalidos.sum()\n",
    "            \n",
    "            if n_invalidos > 0:\n",
    "                errores.append(f\"  ‚ùå {fk_col}: {n_invalidos} FK inv√°lidos\")\n",
    "            else:\n",
    "                logger.log(f\"  ‚úì {fk_col}: OK\")\n",
    "    \n",
    "    if errores:\n",
    "        logger.log(\"\\n‚ö†Ô∏è  ERRORES DE INTEGRIDAD ENCONTRADOS:\")\n",
    "        for error in errores:\n",
    "            logger.log(error)\n",
    "        raise ValueError(\"Integridad referencial violada\")\n",
    "    else:\n",
    "        logger.log(\"  ‚úÖ Integridad referencial OK\")\n",
    "\n",
    "\n",
    "def validar_calidad_datos(fact, dims):\n",
    "    \"\"\"Validar calidad de datos\"\"\"\n",
    "    logger.log(\"\\nValidando calidad de datos...\")\n",
    "    \n",
    "    # 1. Registros en fact\n",
    "    logger.log(f\"  Total registros FACT: {len(fact):,}\")\n",
    "    \n",
    "    # 2. Duplicados\n",
    "    duplicados = fact.duplicated(subset=['id_nino', 'id_tiempo']).sum()\n",
    "    if duplicados > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  Duplicados encontrados: {duplicados}\")\n",
    "    else:\n",
    "        logger.log(f\"  ‚úì Sin duplicados\")\n",
    "    \n",
    "    # 3. Missings en FK\n",
    "    for col in ['id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre']:\n",
    "        if col in fact.columns:\n",
    "            missing = fact[col].isna().sum()\n",
    "            if missing > 0:\n",
    "                logger.log(f\"  ‚ö†Ô∏è  {col}: {missing} missings ({missing/len(fact)*100:.1f}%)\")\n",
    "    \n",
    "    # 4. Estad√≠sticas de dimensiones\n",
    "    logger.log(f\"\\nDimensiones:\")\n",
    "    logger.log(f\"  DIM_TIEMPO:    {len(dims['dim_tiempo']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_GEOGRAFIA: {len(dims['dim_geografia']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_HOGAR:     {len(dims['dim_hogar']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_MADRE:     {len(dims['dim_madre']):>6,} registros\")\n",
    "    logger.log(f\"  DIM_NINO:      {len(dims['dim_nino']):>6,} registros\")\n",
    "    \n",
    "    logger.log(\"\\n  ‚úÖ Validaci√≥n de calidad completada\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR EN SQLITE\n",
    "# ============================================================\n",
    "\n",
    "def crear_esquema_sqlite(conn):\n",
    "    \"\"\"Crear estructura de tablas con DDL\"\"\"\n",
    "    logger.log(\"\\nCreando esquema SQLite...\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # DDL para cada tabla (simplificado - SQLite infiere tipos)\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS fact_anemia\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_tiempo\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_geografia\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_hogar\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_madre\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_nino\")\n",
    "    \n",
    "    conn.commit()\n",
    "    logger.log(\"  ‚úì Tablas anteriores eliminadas (si exist√≠an)\")\n",
    "\n",
    "\n",
    "def guardar_en_sqlite(fact, dims, db_path):\n",
    "    \"\"\"Guardar modelo estrella en SQLite\"\"\"\n",
    "    logger.log(f\"\\nGuardando en SQLite: {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Crear esquema\n",
    "    crear_esquema_sqlite(conn)\n",
    "    \n",
    "    # Guardar dimensiones\n",
    "    logger.log(\"  Guardando dimensiones...\")\n",
    "    dims['dim_tiempo'].to_sql('dim_tiempo', conn, if_exists='replace', index=False)\n",
    "    dims['dim_geografia'].to_sql('dim_geografia', conn, if_exists='replace', index=False)\n",
    "    dims['dim_hogar'].to_sql('dim_hogar', conn, if_exists='replace', index=False)\n",
    "    dims['dim_madre'].to_sql('dim_madre', conn, if_exists='replace', index=False)\n",
    "    dims['dim_nino'].to_sql('dim_nino', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Guardar hechos (en chunks)\n",
    "    logger.log(\"  Guardando tabla de hechos...\")\n",
    "    fact.to_sql('fact_anemia', conn, if_exists='replace', \n",
    "                index=False, chunksize=config.CHUNK_SIZE)\n",
    "    \n",
    "    # Crear √≠ndices\n",
    "    logger.log(\"  Creando √≠ndices...\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    indices = [\n",
    "        \"CREATE INDEX idx_fact_tiempo ON fact_anemia(id_tiempo)\",\n",
    "        \"CREATE INDEX idx_fact_geografia ON fact_anemia(id_geografia)\",\n",
    "        \"CREATE INDEX idx_fact_hogar ON fact_anemia(id_hogar)\",\n",
    "        \"CREATE INDEX idx_fact_madre ON fact_anemia(id_madre)\",\n",
    "        \"CREATE INDEX idx_fact_nino ON fact_anemia(id_nino)\",\n",
    "        \"CREATE INDEX idx_fact_anemia ON fact_anemia(tiene_anemia)\",\n",
    "        \"CREATE INDEX idx_dim_geo_dept ON dim_geografia(departamento)\",\n",
    "        \"CREATE INDEX idx_dim_tiempo_anio ON dim_tiempo(anio)\"\n",
    "    ]\n",
    "    \n",
    "    for idx_sql in indices:\n",
    "        try:\n",
    "            cursor.execute(idx_sql)\n",
    "        except Exception as e:\n",
    "            logger.log(f\"    ‚ö†Ô∏è  Error creando √≠ndice: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    logger.log(\"  ‚úÖ Guardado en SQLite completado\")\n",
    "\n",
    "\n",
    "def guardar_backup_parquet(fact, dims):\n",
    "    \"\"\"Backup en Parquet\"\"\"\n",
    "    if not config.BACKUP_PARQUET:\n",
    "        return\n",
    "    \n",
    "    logger.log(f\"\\nCreando backup Parquet en {config.BACKUP_DIR}...\")\n",
    "    \n",
    "    fact.to_parquet(config.BACKUP_DIR / \"fact_anemia.parquet\", index=False)\n",
    "    dims['dim_tiempo'].to_parquet(config.BACKUP_DIR / \"dim_tiempo.parquet\", index=False)\n",
    "    dims['dim_geografia'].to_parquet(config.BACKUP_DIR / \"dim_geografia.parquet\", index=False)\n",
    "    dims['dim_hogar'].to_parquet(config.BACKUP_DIR / \"dim_hogar.parquet\", index=False)\n",
    "    dims['dim_madre'].to_parquet(config.BACKUP_DIR / \"dim_madre.parquet\", index=False)\n",
    "    dims['dim_nino'].to_parquet(config.BACKUP_DIR / \"dim_nino.parquet\", index=False)\n",
    "    \n",
    "    logger.log(\"  ‚úÖ Backup Parquet completado\")\n",
    "\n",
    "\n",
    "def generar_metadata(fact, dims):\n",
    "    \"\"\"Generar metadata del warehouse\"\"\"\n",
    "    logger.log(\"\\nGenerando metadata...\")\n",
    "    \n",
    "    metadata = {\n",
    "        'fecha_creacion': datetime.now().isoformat(),\n",
    "        'registros': {\n",
    "            'fact_anemia': len(fact),\n",
    "            'dim_tiempo': len(dims['dim_tiempo']),\n",
    "            'dim_geografia': len(dims['dim_geografia']),\n",
    "            'dim_hogar': len(dims['dim_hogar']),\n",
    "            'dim_madre': len(dims['dim_madre']),\n",
    "            'dim_nino': len(dims['dim_nino'])\n",
    "        },\n",
    "        'columnas': {\n",
    "            'fact_anemia': list(fact.columns),\n",
    "            'dim_tiempo': list(dims['dim_tiempo'].columns),\n",
    "            'dim_geografia': list(dims['dim_geografia'].columns),\n",
    "            'dim_hogar': list(dims['dim_hogar'].columns),\n",
    "            'dim_madre': list(dims['dim_madre'].columns),\n",
    "            'dim_nino': list(dims['dim_nino'].columns)\n",
    "        },\n",
    "        'prevalencia_anemia': float(fact['tiene_anemia'].mean() * 100) if 'tiene_anemia' in fact.columns else None\n",
    "    }\n",
    "    \n",
    "    # Guardar como JSON\n",
    "    with open(config.METADATA_DIR / 'estadisticas.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    logger.log(\"  ‚úì Metadata guardada\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline completo ETL\"\"\"\n",
    "    \n",
    "    logger.log(\"=\"*60)\n",
    "    logger.log(\"INICIANDO GENERACI√ìN DE MODELO ESTRELLA\")\n",
    "    logger.log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. EXTRACT\n",
    "        logger.log(f\"\\n1. Cargando CSV: {config.CSV_INPUT}\")\n",
    "        df = pd.read_csv(config.CSV_INPUT, encoding='utf-8-sig')\n",
    "        logger.log(f\"   ‚úì {len(df):,} registros cargados\")\n",
    "        logger.log(f\"   ‚úì {df.shape[1]} columnas\")\n",
    "        \n",
    "        # 2. TRANSFORM - Crear dimensiones\n",
    "        logger.log(\"\\n2. Creando dimensiones...\")\n",
    "        dim_tiempo = crear_dim_tiempo(df)\n",
    "        dim_geografia = crear_dim_geografia(df)\n",
    "        dim_hogar = crear_dim_hogar(df)\n",
    "        dim_madre = crear_dim_madre(df)\n",
    "        dim_nino = crear_dim_nino(df)\n",
    "        \n",
    "        dims = {\n",
    "            'dim_tiempo': dim_tiempo,\n",
    "            'dim_geografia': dim_geografia,\n",
    "            'dim_hogar': dim_hogar,\n",
    "            'dim_madre': dim_madre,\n",
    "            'dim_nino': dim_nino\n",
    "        }\n",
    "        \n",
    "        # 3. TRANSFORM - Crear hechos\n",
    "        logger.log(\"\\n3. Creando tabla de hechos...\")\n",
    "        fact = crear_fact_anemia(df, dim_tiempo, dim_geografia, \n",
    "                                dim_hogar, dim_madre, dim_nino)\n",
    "        \n",
    "        # 4. VALIDACIONES\n",
    "        logger.log(\"\\n4. Validando modelo...\")\n",
    "        validar_integridad_referencial(fact, dims)\n",
    "        validar_calidad_datos(fact, dims)\n",
    "        \n",
    "        # 5. LOAD\n",
    "        logger.log(\"\\n5. Cargando a Data Warehouse...\")\n",
    "        guardar_en_sqlite(fact, dims, config.DB_OUTPUT)\n",
    "        guardar_backup_parquet(fact, dims)\n",
    "        generar_metadata(fact, dims)\n",
    "        \n",
    "        # FIN\n",
    "        logger.log(\"\\n\" + \"=\"*60)\n",
    "        logger.log(\"‚úÖ MODELO ESTRELLA GENERADO EXITOSAMENTE\")\n",
    "        logger.log(\"=\"*60)\n",
    "        logger.log(f\"\\nüìÅ Archivos generados:\")\n",
    "        logger.log(f\"   ‚Ä¢ SQLite:  {config.DB_OUTPUT}\")\n",
    "        logger.log(f\"   ‚Ä¢ Metadata: {config.METADATA_DIR}\")\n",
    "        logger.log(f\"   ‚Ä¢ Backup:   {config.BACKUP_DIR}\")\n",
    "        \n",
    "        # Estad√≠sticas finales\n",
    "        tamanio_db = Path(config.DB_OUTPUT).stat().st_size / (1024*1024)\n",
    "        logger.log(f\"\\nüìä Estad√≠sticas:\")\n",
    "        logger.log(f\"   ‚Ä¢ Tama√±o DB: {tamanio_db:.2f} MB\")\n",
    "        logger.log(f\"   ‚Ä¢ Total registros FACT: {len(fact):,}\")\n",
    "        logger.log(f\"   ‚Ä¢ Prevalencia anemia: {fact['tiene_anemia'].mean()*100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.log(traceback.format_exc())\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        logger.finalizar()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470ecfe-1635-4d8c-8f3a-3434aba0ab28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bceb36-9625-4c75-aaa2-55df5081836d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd8317-6aca-4378-98be-4154bd77b2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9fbd7-0820-40fe-b090-c2e228515e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337f6bd-1b77-4898-8ea8-421a548a8a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e28a70-9fcf-4748-8c54-82c3818cec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-21 00:34:35] ============================================================\n",
      "[2025-11-21 00:34:35] GENERACI√ìN DE MODELO ESTRELLA - VERSI√ìN FINAL\n",
      "[2025-11-21 00:34:35] ============================================================\n",
      "[2025-11-21 00:34:35] \n",
      "1. Cargando: D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\n",
      "[2025-11-21 00:34:35]    ‚úì 131,470 registros, 42 columnas\n",
      "[2025-11-21 00:34:35] \n",
      "2. Creando dimensiones...\n",
      "[2025-11-21 00:34:35] Creando DIM_TIEMPO...\n",
      "[2025-11-21 00:34:35]   ‚úì 10 registros creados\n",
      "[2025-11-21 00:34:35] Creando DIM_GEOGRAFIA...\n",
      "[2025-11-21 00:34:35]   ‚úì 8373 registros creados\n",
      "[2025-11-21 00:34:35] Creando DIM_HOGAR...\n",
      "[2025-11-21 00:34:35]   ‚úì 94921 registros creados\n",
      "[2025-11-21 00:34:35] Creando DIM_MADRE...\n",
      "[2025-11-21 00:34:35]   ‚úì 99928 registros creados\n",
      "[2025-11-21 00:34:35] Creando DIM_NINO...\n",
      "[2025-11-21 00:34:35]   ‚ö†Ô∏è  26552 duplicados HHID+HC0 encontrados, eliminando...\n",
      "[2025-11-21 00:34:35]   ‚úì 104918 registros creados\n",
      "[2025-11-21 00:34:35] \n",
      "3. Creando hechos...\n",
      "[2025-11-21 00:34:35] Creando FACT_ANEMIA...\n",
      "[2025-11-21 00:34:35]   ‚ö†Ô∏è  24988 duplicados eliminados\n",
      "[2025-11-21 00:34:36]   ‚úì 106482 registros creados\n",
      "[2025-11-21 00:34:36] \n",
      "4. Validando...\n",
      "[2025-11-21 00:34:36] \n",
      "Validando integridad referencial...\n",
      "[2025-11-21 00:34:36]   ‚úì id_tiempo: OK\n",
      "[2025-11-21 00:34:36]   ‚úì id_geografia: OK\n",
      "[2025-11-21 00:34:36]   ‚úì id_hogar: OK\n",
      "[2025-11-21 00:34:36]   ‚úì id_madre: OK\n",
      "[2025-11-21 00:34:36]   ‚úì id_nino: OK\n",
      "[2025-11-21 00:34:36]   ‚úÖ Integridad OK\n",
      "[2025-11-21 00:34:36] \n",
      "Validando calidad...\n",
      "[2025-11-21 00:34:36]   FACT: 106,482 registros\n",
      "[2025-11-21 00:34:36]   Duplicados: 0\n",
      "[2025-11-21 00:34:36] \n",
      "Dimensiones:\n",
      "[2025-11-21 00:34:36]   DIM_TIEMPO:        10\n",
      "[2025-11-21 00:34:36]   DIM_GEOGRAFIA:  8,373\n",
      "[2025-11-21 00:34:36]   DIM_HOGAR:     94,921\n",
      "[2025-11-21 00:34:36]   DIM_MADRE:     99,928\n",
      "[2025-11-21 00:34:36]   DIM_NINO:      104,918\n",
      "[2025-11-21 00:34:36] \n",
      "5. Guardando...\n",
      "[2025-11-21 00:34:36] \n",
      "Guardando en SQLite: D:\\Data_Warehouse\\anemia_dwh.db\n",
      "[2025-11-21 00:34:36]   Guardando dimensiones...\n",
      "[2025-11-21 00:34:37]   Guardando hechos...\n",
      "[2025-11-21 00:34:38]   Creando √≠ndices...\n",
      "[2025-11-21 00:34:38] \n",
      "  ‚úì Tablas creadas: ['dim_tiempo', 'dim_geografia', 'dim_hogar', 'dim_madre', 'dim_nino', 'fact_anemia']\n",
      "[2025-11-21 00:34:38]   ‚úì fact_anemia: 106,482 registros\n",
      "[2025-11-21 00:34:38]   ‚úì dim_tiempo: 10 registros\n",
      "[2025-11-21 00:34:38]   ‚úì dim_geografia: 8,373 registros\n",
      "[2025-11-21 00:34:38]   ‚úì dim_hogar: 94,921 registros\n",
      "[2025-11-21 00:34:38]   ‚úì dim_madre: 99,928 registros\n",
      "[2025-11-21 00:34:38]   ‚úì dim_nino: 104,918 registros\n",
      "[2025-11-21 00:34:38]   ‚úÖ Guardado completado\n",
      "[2025-11-21 00:34:38] \n",
      "6. Verificaci√≥n final...\n",
      "[2025-11-21 00:34:38] \n",
      "üìä Casos por a√±o:\n",
      "[2025-11-21 00:34:38]  anio  casos  con_anemia  prevalencia\n",
      " 2015  12551      5829.0        46.44\n",
      " 2016  10949      5141.0        46.95\n",
      " 2017  11364      5292.0        46.57\n",
      " 2018  11703      5382.0        45.99\n",
      " 2019  10843      4602.0        42.44\n",
      " 2020   6318      2588.0        40.96\n",
      " 2021  11270      4754.0        42.18\n",
      " 2022  10982      5003.0        45.56\n",
      " 2023  10347      4708.0        45.50\n",
      " 2024  10154      4670.0        45.99\n",
      "[2025-11-21 00:34:38] \n",
      "‚úÖ COMPLETADO\n",
      "[2025-11-21 00:34:38]    ‚Ä¢ Tama√±o DB: 26.32 MB\n",
      "[2025-11-21 00:34:38]    ‚Ä¢ Total registros: 106,482\n",
      "[2025-11-21 00:34:38]    ‚Ä¢ Prevalencia: 45.05%\n",
      "[2025-11-21 00:34:38] \n",
      "============================================================\n",
      "[2025-11-21 00:34:38] Proceso completado en 3.30 segundos\n",
      "[2025-11-21 00:34:38] ============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GENERADOR DE MODELO ESTRELLA - ANEMIA INFANTIL ENDES (VERSI√ìN FINAL)\n",
    "Convierte CSV consolidado ‚Üí SQLite con esquema estrella\n",
    "CORRECCIONES:\n",
    "- dim_tiempo: 1 registro por a√±o\n",
    "- Manejo de duplicados en dim_nino y fact_anemia\n",
    "- Funci√≥n crear_fact_anemia sin duplicaci√≥n\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ============================================================\n",
    "class Config:\n",
    "    CSV_INPUT = r\"D:\\Bases_train_test\\endes_2015_2024_consolidado.csv\"\n",
    "    DB_OUTPUT = r\"D:\\Data_Warehouse\\anemia_dwh.db\"\n",
    "    METADATA_DIR = Path(r\"D:\\Data_Warehouse\\metadata\")\n",
    "    BACKUP_DIR = Path(r\"D:\\Data_Warehouse\\backup\")\n",
    "    \n",
    "    METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    CHUNK_SIZE = 5000\n",
    "    BACKUP_PARQUET = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================\n",
    "# LOGGER\n",
    "# ============================================================\n",
    "class Logger:\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "    def log(self, mensaje):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        msg = f\"[{timestamp}] {mensaje}\"\n",
    "        print(msg)\n",
    "        with open(self.log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(msg + '\\n')\n",
    "    \n",
    "    def finalizar(self):\n",
    "        duracion = (datetime.now() - self.start_time).total_seconds()\n",
    "        self.log(f\"\\n{'='*60}\")\n",
    "        self.log(f\"Proceso completado en {duracion:.2f} segundos\")\n",
    "        self.log(f\"{'='*60}\")\n",
    "\n",
    "log_file = config.METADATA_DIR / f\"log_etl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "logger = Logger(log_file)\n",
    "\n",
    "# ============================================================\n",
    "# DIMENSIONES\n",
    "# ============================================================\n",
    "\n",
    "def crear_dim_tiempo(df):\n",
    "    \"\"\"Dimensi√≥n TIEMPO - 1 registro por a√±o\"\"\"\n",
    "    logger.log(\"Creando DIM_TIEMPO...\")\n",
    "    \n",
    "    anios = sorted(df['ANIO'].dropna().unique())\n",
    "    \n",
    "    dim_tiempo = []\n",
    "    for anio in anios:\n",
    "        anio_int = int(anio)\n",
    "        dim_tiempo.append({\n",
    "            'id_tiempo': anio_int * 100 + 6,\n",
    "            'anio': anio_int,\n",
    "            'mes': 6,\n",
    "            'trimestre': 2,\n",
    "            'semestre': 1,\n",
    "            'nombre_mes': 'Junio',\n",
    "            'quinquenio': f\"{(anio_int//5)*5}-{(anio_int//5)*5+4}\",\n",
    "            'periodo_covid': int(anio_int in [2020, 2021])\n",
    "        })\n",
    "    \n",
    "    df_tiempo = pd.DataFrame(dim_tiempo)\n",
    "    logger.log(f\"  ‚úì {len(df_tiempo)} registros creados\")\n",
    "    return df_tiempo\n",
    "\n",
    "\n",
    "def crear_dim_geografia(df):\n",
    "    \"\"\"Dimensi√≥n GEOGRAF√çA\"\"\"\n",
    "    logger.log(\"Creando DIM_GEOGRAFIA...\")\n",
    "    \n",
    "    geo_cols = ['HV024', 'HV025', 'HV040']\n",
    "    df_geo = df[geo_cols].drop_duplicates().reset_index(drop=True)\n",
    "    df_geo['id_geografia'] = range(1, len(df_geo) + 1)\n",
    "    \n",
    "    mapa_region = {\n",
    "        'tumbes': 'Costa', 'piura': 'Costa', 'lambayeque': 'Costa', \n",
    "        'la libertad': 'Costa', 'ancash': 'Costa', 'lima': 'Costa',\n",
    "        'ica': 'Costa', 'arequipa': 'Costa', 'moquegua': 'Costa', \n",
    "        'tacna': 'Costa', 'callao': 'Costa',\n",
    "        'cajamarca': 'Sierra', 'huanuco': 'Sierra', 'pasco': 'Sierra',\n",
    "        'junin': 'Sierra', 'huancavelica': 'Sierra', 'ayacucho': 'Sierra',\n",
    "        'apurimac': 'Sierra', 'cusco': 'Sierra', 'puno': 'Sierra',\n",
    "        'loreto': 'Selva', 'amazonas': 'Selva', 'san martin': 'Selva',\n",
    "        'ucayali': 'Selva', 'madre de dios': 'Selva'\n",
    "    }\n",
    "    \n",
    "    df_geo['region_natural'] = df_geo['HV024'].str.lower().str.strip().map(mapa_region).fillna('Otro')\n",
    "    \n",
    "    df_geo['rango_altitud'] = pd.cut(\n",
    "        df_geo['HV040'],\n",
    "        bins=[-1, 500, 1500, 2500, 5000],\n",
    "        labels=['<500m', '500-1500m', '1500-2500m', '>2500m']\n",
    "    ).astype(str)\n",
    "    \n",
    "    df_geo.rename(columns={\n",
    "        'HV024': 'departamento',\n",
    "        'HV025': 'area_residencia',\n",
    "        'HV040': 'altitud_msnm'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df_geo = df_geo[['id_geografia', 'departamento', 'region_natural', \n",
    "                     'area_residencia', 'altitud_msnm', 'rango_altitud']]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_geo)} registros creados\")\n",
    "    return df_geo\n",
    "\n",
    "\n",
    "def crear_dim_hogar(df):\n",
    "    \"\"\"Dimensi√≥n HOGAR\"\"\"\n",
    "    logger.log(\"Creando DIM_HOGAR...\")\n",
    "    \n",
    "    hogar_cols = ['HHID', 'HV009', 'HV271', 'V190', 'HV206', 'HV201', 'HV205', 'HV237']\n",
    "    hogar_cols_disp = [col for col in hogar_cols if col in df.columns]\n",
    "    \n",
    "    df_hogar = df[hogar_cols_disp].drop_duplicates(subset='HHID').reset_index(drop=True)\n",
    "    df_hogar['id_hogar'] = range(1, len(df_hogar) + 1)\n",
    "    \n",
    "    if 'HV009' in df_hogar.columns:\n",
    "        df_hogar['categoria_tamano'] = pd.cut(\n",
    "            df_hogar['HV009'],\n",
    "            bins=[0, 3, 5, 100],\n",
    "            labels=['Peque√±o', 'Mediano', 'Grande']\n",
    "        ).astype(str)\n",
    "    \n",
    "    rename_map = {\n",
    "        'HV009': 'num_miembros', 'HV271': 'indice_riqueza_num',\n",
    "        'V190': 'quintil_riqueza', 'HV206': 'tiene_electricidad',\n",
    "        'HV201': 'fuente_agua', 'HV205': 'tipo_saneamiento', 'HV237': 'trata_agua'\n",
    "    }\n",
    "    df_hogar.rename(columns={k: v for k, v in rename_map.items() if k in df_hogar.columns}, inplace=True)\n",
    "    \n",
    "    cols = ['id_hogar', 'HHID'] + [c for c in df_hogar.columns if c not in ['id_hogar', 'HHID']]\n",
    "    df_hogar = df_hogar[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_hogar)} registros creados\")\n",
    "    return df_hogar\n",
    "\n",
    "\n",
    "def crear_dim_madre(df):\n",
    "    \"\"\"Dimensi√≥n MADRE\"\"\"\n",
    "    logger.log(\"Creando DIM_MADRE...\")\n",
    "    \n",
    "    madre_cols = ['CASEID', 'V012', 'V106', 'V133', 'V025']\n",
    "    madre_cols_disp = [col for col in madre_cols if col in df.columns]\n",
    "    \n",
    "    df_madre = df[madre_cols_disp].drop_duplicates(subset='CASEID').reset_index(drop=True)\n",
    "    df_madre['id_madre'] = range(1, len(df_madre) + 1)\n",
    "    \n",
    "    if 'V012' in df_madre.columns:\n",
    "        df_madre['rango_edad'] = pd.cut(df_madre['V012'], bins=[0, 20, 35, 100], \n",
    "                                         labels=['<20', '20-34', '35+']).astype(str)\n",
    "        df_madre['es_madre_adolescente'] = (df_madre['V012'] < 20).astype(int)\n",
    "    \n",
    "    if 'V133' in df_madre.columns:\n",
    "        df_madre['categoria_educacion'] = pd.cut(df_madre['V133'], bins=[-1, 6, 12, 100],\n",
    "                                                  labels=['Baja', 'Media', 'Alta']).astype(str)\n",
    "    \n",
    "    rename_map = {'V012': 'edad_actual', 'V106': 'nivel_educativo', \n",
    "                  'V133': 'anios_educacion', 'V025': 'area_residencia_madre'}\n",
    "    df_madre.rename(columns={k: v for k, v in rename_map.items() if k in df_madre.columns}, inplace=True)\n",
    "    \n",
    "    cols = ['id_madre', 'CASEID'] + [c for c in df_madre.columns if c not in ['id_madre', 'CASEID']]\n",
    "    df_madre = df_madre[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_madre)} registros creados\")\n",
    "    return df_madre\n",
    "\n",
    "\n",
    "def crear_dim_nino(df):\n",
    "    \"\"\"Dimensi√≥n NI√ëO - Elimina duplicados\"\"\"\n",
    "    logger.log(\"Creando DIM_NINO...\")\n",
    "    \n",
    "    nino_cols = ['HHID', 'HC0', 'HC1', 'HC27', 'BORD', 'HC70', 'HW71']\n",
    "    nino_cols_disp = [col for col in nino_cols if col in df.columns]\n",
    "    \n",
    "    # Eliminar duplicados ANTES de crear dimensi√≥n\n",
    "    duplicados = df.duplicated(subset=['HHID', 'HC0']).sum()\n",
    "    if duplicados > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  {duplicados} duplicados HHID+HC0 encontrados, eliminando...\")\n",
    "    \n",
    "    df_nino = df[nino_cols_disp].drop_duplicates(subset=['HHID', 'HC0'], keep='first').copy()\n",
    "    df_nino['id_nino'] = range(1, len(df_nino) + 1)\n",
    "    \n",
    "    if 'HC1' in df_nino.columns:\n",
    "        df_nino['rango_edad'] = pd.cut(df_nino['HC1'], bins=[5, 12, 18, 24, 36],\n",
    "                                        labels=['6-11m', '12-17m', '18-23m', '24-35m'],\n",
    "                                        right=False).astype(str)\n",
    "    \n",
    "    if 'HC70' in df_nino.columns:\n",
    "        df_nino['tiene_desnutricion_cronica'] = (df_nino['HC70'] < -2).astype(int)\n",
    "    \n",
    "    if 'HW71' in df_nino.columns:\n",
    "        df_nino['tiene_bajo_peso'] = (df_nino['HW71'] < -2).astype(int)\n",
    "    \n",
    "    if 'BORD' in df_nino.columns:\n",
    "        df_nino['categoria_orden'] = pd.cut(df_nino['BORD'], bins=[0, 1, 3, 100],\n",
    "                                            labels=['Primog√©nito', '2-3', '4+']).astype(str)\n",
    "    \n",
    "    rename_map = {'HC1': 'edad_meses', 'HC27': 'sexo', 'BORD': 'orden_nacimiento'}\n",
    "    df_nino.rename(columns={k: v for k, v in rename_map.items() if k in df_nino.columns}, inplace=True)\n",
    "    \n",
    "    cols = ['id_nino', 'HHID', 'HC0'] + [c for c in df_nino.columns if c not in ['id_nino', 'HHID', 'HC0']]\n",
    "    df_nino = df_nino[cols]\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(df_nino)} registros creados\")\n",
    "    return df_nino\n",
    "\n",
    "\n",
    "def crear_fact_anemia(df, dim_tiempo, dim_geo, dim_hogar, dim_madre, dim_nino):\n",
    "    \"\"\"Tabla de HECHOS - Sin duplicados\"\"\"\n",
    "    logger.log(\"Creando FACT_ANEMIA...\")\n",
    "    \n",
    "    fact = df.copy()\n",
    "    \n",
    "    # 1. ELIMINAR DUPLICADOS PRIMERO\n",
    "    registros_antes = len(fact)\n",
    "    fact = fact.drop_duplicates(subset=['HHID', 'HC0', 'ANIO'], keep='first')\n",
    "    registros_despues = len(fact)\n",
    "    \n",
    "    if registros_antes != registros_despues:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  {registros_antes - registros_despues} duplicados eliminados\")\n",
    "    \n",
    "    # 2. Mapear FK\n",
    "    fact['id_tiempo'] = fact['ANIO'].astype(int) * 100 + 6\n",
    "    \n",
    "    # 3. Geograf√≠a\n",
    "    fact = fact.merge(\n",
    "        dim_geo[['id_geografia', 'departamento', 'area_residencia', 'altitud_msnm']],\n",
    "        left_on=['HV024', 'HV025', 'HV040'],\n",
    "        right_on=['departamento', 'area_residencia', 'altitud_msnm'],\n",
    "        how='left'\n",
    "    ).drop(columns=['departamento', 'area_residencia', 'altitud_msnm'])\n",
    "    \n",
    "    # 4. Hogar\n",
    "    fact = fact.merge(dim_hogar[['id_hogar', 'HHID']], on='HHID', how='left')\n",
    "    \n",
    "    # 5. Madre\n",
    "    fact = fact.merge(dim_madre[['id_madre', 'CASEID']], on='CASEID', how='left')\n",
    "    \n",
    "    # 6. Ni√±o\n",
    "    fact = fact.merge(dim_nino[['id_nino', 'HHID', 'HC0']], on=['HHID', 'HC0'], how='left')\n",
    "    \n",
    "    # 7. Verificar duplicados finales\n",
    "    dup_final = fact.duplicated(subset=['id_nino', 'id_tiempo']).sum()\n",
    "    if dup_final > 0:\n",
    "        logger.log(f\"  ‚ö†Ô∏è  {dup_final} duplicados finales, eliminando...\")\n",
    "        fact = fact.drop_duplicates(subset=['id_nino', 'id_tiempo'], keep='first')\n",
    "    \n",
    "    # 8. Seleccionar columnas\n",
    "    fact_cols = [\n",
    "        'id_nino', 'id_tiempo', 'id_geografia', 'id_hogar', 'id_madre',\n",
    "        'HHID', 'CASEID', 'HC0',\n",
    "        'HW2', 'HW3', 'HC70', 'HW70', 'HW71', 'HW72', 'HW73',\n",
    "        'ANEMIA', 'HC57', 'PESO',\n",
    "        'HC55', 'HV015', 'HV103'\n",
    "    ]\n",
    "    \n",
    "    fact_cols_disp = [col for col in fact_cols if col in fact.columns]\n",
    "    fact = fact[fact_cols_disp]\n",
    "    \n",
    "    # 9. Renombrar\n",
    "    rename_map = {\n",
    "        'HW2': 'peso_kg', 'HW3': 'talla_cm', 'HC70': 'z_talla_edad',\n",
    "        'HW70': 'z_talla_edad_alt', 'HW71': 'z_peso_edad', 'HW72': 'z_peso_talla',\n",
    "        'HW73': 'z_imc', 'ANEMIA': 'tiene_anemia', 'HC57': 'nivel_anemia',\n",
    "        'PESO': 'peso_muestral', 'HC55': 'medicion_valida',\n",
    "        'HV015': 'cuestionario_ok', 'HV103': 'durmio_anoche'\n",
    "    }\n",
    "    fact.rename(columns={k: v for k, v in rename_map.items() if k in fact.columns}, inplace=True)\n",
    "    \n",
    "    logger.log(f\"  ‚úì {len(fact)} registros creados\")\n",
    "    return fact\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# VALIDACIONES\n",
    "# ============================================================\n",
    "\n",
    "def validar_integridad_referencial(fact, dims):\n",
    "    logger.log(\"\\nValidando integridad referencial...\")\n",
    "    \n",
    "    checks = [\n",
    "        ('id_tiempo', dims['dim_tiempo'], 'id_tiempo'),\n",
    "        ('id_geografia', dims['dim_geografia'], 'id_geografia'),\n",
    "        ('id_hogar', dims['dim_hogar'], 'id_hogar'),\n",
    "        ('id_madre', dims['dim_madre'], 'id_madre'),\n",
    "        ('id_nino', dims['dim_nino'], 'id_nino')\n",
    "    ]\n",
    "    \n",
    "    errores = []\n",
    "    for fk_col, dim_df, dim_pk in checks:\n",
    "        if fk_col in fact.columns:\n",
    "            invalidos = ~fact[fk_col].isin(dim_df[dim_pk])\n",
    "            n_invalidos = invalidos.sum()\n",
    "            \n",
    "            if n_invalidos > 0:\n",
    "                errores.append(f\"  ‚ùå {fk_col}: {n_invalidos} inv√°lidos\")\n",
    "            else:\n",
    "                logger.log(f\"  ‚úì {fk_col}: OK\")\n",
    "    \n",
    "    if errores:\n",
    "        logger.log(\"\\n‚ö†Ô∏è ERRORES:\")\n",
    "        for e in errores:\n",
    "            logger.log(e)\n",
    "    else:\n",
    "        logger.log(\"  ‚úÖ Integridad OK\")\n",
    "\n",
    "\n",
    "def validar_calidad_datos(fact, dims):\n",
    "    logger.log(\"\\nValidando calidad...\")\n",
    "    logger.log(f\"  FACT: {len(fact):,} registros\")\n",
    "    \n",
    "    dup = fact.duplicated(subset=['id_nino', 'id_tiempo']).sum()\n",
    "    logger.log(f\"  Duplicados: {dup}\")\n",
    "    \n",
    "    logger.log(f\"\\nDimensiones:\")\n",
    "    logger.log(f\"  DIM_TIEMPO:    {len(dims['dim_tiempo']):>6,}\")\n",
    "    logger.log(f\"  DIM_GEOGRAFIA: {len(dims['dim_geografia']):>6,}\")\n",
    "    logger.log(f\"  DIM_HOGAR:     {len(dims['dim_hogar']):>6,}\")\n",
    "    logger.log(f\"  DIM_MADRE:     {len(dims['dim_madre']):>6,}\")\n",
    "    logger.log(f\"  DIM_NINO:      {len(dims['dim_nino']):>6,}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GUARDAR\n",
    "# ============================================================\n",
    "\n",
    "def guardar_en_sqlite(fact, dims, db_path):\n",
    "    logger.log(f\"\\nGuardando en SQLite: {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Eliminar tablas anteriores\n",
    "    for tabla in ['fact_anemia', 'dim_tiempo', 'dim_geografia', 'dim_hogar', 'dim_madre', 'dim_nino']:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {tabla}\")\n",
    "    conn.commit()\n",
    "    \n",
    "    # Guardar dimensiones\n",
    "    logger.log(\"  Guardando dimensiones...\")\n",
    "    dims['dim_tiempo'].to_sql('dim_tiempo', conn, if_exists='replace', index=False)\n",
    "    dims['dim_geografia'].to_sql('dim_geografia', conn, if_exists='replace', index=False)\n",
    "    dims['dim_hogar'].to_sql('dim_hogar', conn, if_exists='replace', index=False)\n",
    "    dims['dim_madre'].to_sql('dim_madre', conn, if_exists='replace', index=False)\n",
    "    dims['dim_nino'].to_sql('dim_nino', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Guardar hechos\n",
    "    logger.log(\"  Guardando hechos...\")\n",
    "    fact.to_sql('fact_anemia', conn, if_exists='replace', index=False, chunksize=5000)\n",
    "    \n",
    "    # √çndices\n",
    "    logger.log(\"  Creando √≠ndices...\")\n",
    "    indices = [\n",
    "        \"CREATE INDEX idx_fact_tiempo ON fact_anemia(id_tiempo)\",\n",
    "        \"CREATE INDEX idx_fact_geografia ON fact_anemia(id_geografia)\",\n",
    "        \"CREATE INDEX idx_fact_anemia ON fact_anemia(tiene_anemia)\"\n",
    "    ]\n",
    "    \n",
    "    for idx in indices:\n",
    "        try:\n",
    "            cursor.execute(idx)\n",
    "        except Exception as e:\n",
    "            logger.log(f\"  ‚ö†Ô∏è Error √≠ndice: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    # Verificar\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tablas = cursor.fetchall()\n",
    "    logger.log(f\"\\n  ‚úì Tablas creadas: {[t[0] for t in tablas]}\")\n",
    "    \n",
    "    for tabla in ['fact_anemia', 'dim_tiempo', 'dim_geografia', 'dim_hogar', 'dim_madre', 'dim_nino']:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {tabla}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        logger.log(f\"  ‚úì {tabla}: {count:,} registros\")\n",
    "    \n",
    "    conn.close()\n",
    "    logger.log(\"  ‚úÖ Guardado completado\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    logger.log(\"=\"*60)\n",
    "    logger.log(\"GENERACI√ìN DE MODELO ESTRELLA - VERSI√ìN FINAL\")\n",
    "    logger.log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # 1. Cargar\n",
    "        logger.log(f\"\\n1. Cargando: {config.CSV_INPUT}\")\n",
    "        df = pd.read_csv(config.CSV_INPUT, encoding='utf-8-sig')\n",
    "        logger.log(f\"   ‚úì {len(df):,} registros, {df.shape[1]} columnas\")\n",
    "        \n",
    "        # 2. Dimensiones\n",
    "        logger.log(\"\\n2. Creando dimensiones...\")\n",
    "        dim_tiempo = crear_dim_tiempo(df)\n",
    "        dim_geografia = crear_dim_geografia(df)\n",
    "        dim_hogar = crear_dim_hogar(df)\n",
    "        dim_madre = crear_dim_madre(df)\n",
    "        dim_nino = crear_dim_nino(df)\n",
    "        \n",
    "        dims = {\n",
    "            'dim_tiempo': dim_tiempo,\n",
    "            'dim_geografia': dim_geografia,\n",
    "            'dim_hogar': dim_hogar,\n",
    "            'dim_madre': dim_madre,\n",
    "            'dim_nino': dim_nino\n",
    "        }\n",
    "        \n",
    "        # 3. Hechos\n",
    "        logger.log(\"\\n3. Creando hechos...\")\n",
    "        fact = crear_fact_anemia(df, dim_tiempo, dim_geografia, dim_hogar, dim_madre, dim_nino)\n",
    "        \n",
    "        # 4. Validar\n",
    "        logger.log(\"\\n4. Validando...\")\n",
    "        validar_integridad_referencial(fact, dims)\n",
    "        validar_calidad_datos(fact, dims)\n",
    "        \n",
    "        # 5. Guardar\n",
    "        logger.log(\"\\n5. Guardando...\")\n",
    "        guardar_en_sqlite(fact, dims, config.DB_OUTPUT)\n",
    "        \n",
    "        # Verificaci√≥n final\n",
    "        logger.log(\"\\n6. Verificaci√≥n final...\")\n",
    "        conn = sqlite3.connect(config.DB_OUTPUT)\n",
    "        query = \"\"\"\n",
    "        SELECT t.anio, COUNT(*) as casos, SUM(f.tiene_anemia) as con_anemia,\n",
    "               ROUND(AVG(f.tiene_anemia)*100, 2) as prevalencia\n",
    "        FROM fact_anemia f\n",
    "        JOIN dim_tiempo t ON f.id_tiempo = t.id_tiempo\n",
    "        WHERE f.tiene_anemia IS NOT NULL\n",
    "        GROUP BY t.anio\n",
    "        ORDER BY t.anio\n",
    "        \"\"\"\n",
    "        df_verif = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        logger.log(\"\\nüìä Casos por a√±o:\")\n",
    "        logger.log(df_verif.to_string(index=False))\n",
    "        \n",
    "        # Stats finales\n",
    "        tamanio = Path(config.DB_OUTPUT).stat().st_size / (1024*1024)\n",
    "        logger.log(f\"\\n‚úÖ COMPLETADO\")\n",
    "        logger.log(f\"   ‚Ä¢ Tama√±o DB: {tamanio:.2f} MB\")\n",
    "        logger.log(f\"   ‚Ä¢ Total registros: {len(fact):,}\")\n",
    "        logger.log(f\"   ‚Ä¢ Prevalencia: {fact['tiene_anemia'].mean()*100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.log(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.log(traceback.format_exc())\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        logger.finalizar()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646dcdc-0db0-4141-b839-e1ff7a22cd99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
